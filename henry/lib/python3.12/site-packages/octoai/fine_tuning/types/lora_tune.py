# This file was auto-generated by Fern from our API Definition.

import datetime as dt
import typing

from ...core.datetime_utils import serialize_datetime
from ...core.pydantic_utilities import deep_union_pydantic_dicts, pydantic_v1
from .lora_tune_checkpoint import LoraTuneCheckpoint
from .lora_tune_file import LoraTuneFile


class LoraTune(pydantic_v1.BaseModel):
    """
    A LoRA tune model that includes tune configurations.
    """

    base_checkpoint: LoraTuneCheckpoint = pydantic_v1.Field()
    """
    The base checkpoint used for this LoRA Tune.
    """

    files: typing.List[LoraTuneFile] = pydantic_v1.Field()
    """
    The files used for this LoRA Tune.
    """

    resize_images: typing.Optional[bool] = pydantic_v1.Field(default=None)
    """
    Resize training images to the "native" dimensions of the respective engine prior to tuning. Images will be proportinally resized to such that the shorter edge will fit the native dimension. For Stable Diffusion 1.5, this is 512 pixels, and for Stable Diffusion XL, this is 1024 pixels. If the shorter edge is shorter than the native dimension, no scaling up will be performed.
    """

    seed: typing.Optional[int] = pydantic_v1.Field(default=None)
    """
    The seed used for this LoRA Tune.
    """

    steps: int = pydantic_v1.Field()
    """
    The number of steps used for this LoRA Tune.
    """

    trigger_words: typing.List[str] = pydantic_v1.Field()
    """
    The trigger words used for this tune. As of now, only one trigger word is supported. `List` is used for future-proofing.
    """

    def json(self, **kwargs: typing.Any) -> str:
        kwargs_with_defaults: typing.Any = {"by_alias": True, "exclude_unset": True, **kwargs}
        return super().json(**kwargs_with_defaults)

    def dict(self, **kwargs: typing.Any) -> typing.Dict[str, typing.Any]:
        kwargs_with_defaults_exclude_unset: typing.Any = {"by_alias": True, "exclude_unset": True, **kwargs}
        kwargs_with_defaults_exclude_none: typing.Any = {"by_alias": True, "exclude_none": True, **kwargs}

        return deep_union_pydantic_dicts(
            super().dict(**kwargs_with_defaults_exclude_unset), super().dict(**kwargs_with_defaults_exclude_none)
        )

    class Config:
        frozen = True
        smart_union = True
        extra = pydantic_v1.Extra.allow
        json_encoders = {dt.datetime: serialize_datetime}
