# This file was auto-generated by Fern from our API Definition.

import typing
from json.decoder import JSONDecodeError

from ..core.api_error import ApiError
from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.pydantic_utilities import pydantic_v1
from ..core.request_options import RequestOptions
from .errors.unprocessable_entity_error import UnprocessableEntityError
from .types.http_validation_error import HttpValidationError
from .types.image_encoding import ImageEncoding
from .types.image_generation_request_seed import ImageGenerationRequestSeed
from .types.image_generation_response import ImageGenerationResponse
from .types.ip_adapter_method import IpAdapterMethod
from .types.ip_adapter_mode import IpAdapterMode
from .types.scheduler import Scheduler
from .types.sdxl_styles import SdxlStyles
from .types.video_generation_request_seed import VideoGenerationRequestSeed
from .types.video_generation_response import VideoGenerationResponse

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class ImageGenClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def generate_controlnet_sd15(
        self,
        *,
        prompt: str,
        cfg_scale: typing.Optional[float] = OMIT,
        checkpoint: typing.Optional[str] = OMIT,
        clip_skip: typing.Optional[int] = OMIT,
        controlnet: typing.Optional[str] = OMIT,
        controlnet_conditioning_scale: typing.Optional[float] = OMIT,
        controlnet_early_stop: typing.Optional[float] = OMIT,
        controlnet_image: typing.Optional[str] = OMIT,
        controlnet_preprocess: typing.Optional[bool] = OMIT,
        height: typing.Optional[int] = OMIT,
        high_noise_frac: typing.Optional[float] = OMIT,
        image_encoding: typing.Optional[ImageEncoding] = OMIT,
        init_image: typing.Optional[str] = OMIT,
        ip_adapter_image: typing.Optional[str] = OMIT,
        ip_adapter_mask_image: typing.Optional[str] = OMIT,
        ip_adapter_method: typing.Optional[IpAdapterMethod] = OMIT,
        ip_adapter_mode: typing.Optional[IpAdapterMode] = OMIT,
        ip_adapter_scale: typing.Optional[float] = OMIT,
        loras: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        mask_image: typing.Optional[str] = OMIT,
        negative_prompt: typing.Optional[str] = OMIT,
        negative_prompt2: typing.Optional[str] = OMIT,
        num_images: typing.Optional[int] = OMIT,
        outpainting: typing.Optional[bool] = OMIT,
        prompt2: typing.Optional[str] = OMIT,
        sampler: typing.Optional[Scheduler] = OMIT,
        seed: typing.Optional[ImageGenerationRequestSeed] = OMIT,
        steps: typing.Optional[int] = OMIT,
        strength: typing.Optional[float] = OMIT,
        style_preset: typing.Optional[SdxlStyles] = OMIT,
        textual_inversions: typing.Optional[typing.Dict[str, typing.Optional[str]]] = OMIT,
        transfer_images: typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]] = OMIT,
        use_refiner: typing.Optional[bool] = OMIT,
        vae: typing.Optional[str] = OMIT,
        width: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> ImageGenerationResponse:
        """
        Generate images in response to the given request.

        Parameters
        ----------
        prompt : str
            Text describing the image content to generate.

        cfg_scale : typing.Optional[float]
            Floating-point number represeting how closely to adhere to prompt description. Must be a positive number no greater than 50.0.

        checkpoint : typing.Optional[str]
            [Not supported on SD3] Custom checkpoint to be used during image generation.

        clip_skip : typing.Optional[int]
            [Not supported on SD3] Optionally skip later layers of the text encoder. Higher values lead to more abstract interpretations of the prompt.

        controlnet : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] ControlNet to be used during image generation.

        controlnet_conditioning_scale : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] How strong the effect of the controlnet should be.

        controlnet_early_stop : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] If provided, indicates fraction of steps at which to stop applying controlnet. This can be used to sometimes generate better outputs.

        controlnet_image : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] Controlnet image encoded in b64 string for guiding image generation. Required for controlnet engines.

        controlnet_preprocess : typing.Optional[bool]
            [Not supported on SD3 or IPAdapter] Whether to apply automatic ControlNet preprocessing.

        height : typing.Optional[int]
            Integer representing the height of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        high_noise_frac : typing.Optional[float]
            [Not supported on SD3] Floating-point number that defines the fraction of steps to perform with the base model. Used only by SD XL. Must be greater than or equal to 0.0 and less than or equal to 1.0.

        image_encoding : typing.Optional[ImageEncoding]
            Define which encoding process should be applied before returning the generated image(s).

        init_image : typing.Optional[str]
            Starting point image encoded in b64 string for Image to Image generation mode.

        ip_adapter_image : typing.Optional[str]
            IP Adapter image encoded in b64 string for guiding image generation. Required for ip adapter engines.

        ip_adapter_mask_image : typing.Optional[str]
            b64 encoded mask image for ip adapter. White area should indicate where to paint.

        ip_adapter_method : typing.Optional[IpAdapterMethod]
            Define which ip adapter method should be applied during the image generation. Supported modes: full, style, and composition

        ip_adapter_mode : typing.Optional[IpAdapterMode]
            Define which ip adapter mode should be applied during the image generation. Supported modes: base

        ip_adapter_scale : typing.Optional[float]
            How strong the effect of the ip adapter should be.

        loras : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            [Not supported on SD3] A dictionary of LoRAs to apply. LoRAs as keys and their weights (float) as values.

        mask_image : typing.Optional[str]
            [Not supported on SD3] b64 encoded mask image for inpainting. White area should indicate where to paint.

        negative_prompt : typing.Optional[str]
            Text describing image traits to avoid during generation.

        negative_prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high level description of things to avoid during generation. Used only by SD XL.

        num_images : typing.Optional[int]
            Integer representing how many output images to generate with a single prompt/configuration.

        outpainting : typing.Optional[bool]
            [Not supported on SD3] Whether the request requires outpainting or not. If so, special preprocessing is applied for better results.

        prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high-level description of the image to generate. Used only by SD XL.

        sampler : typing.Optional[Scheduler]
            [Not supported on SD3] Sampler name (also known as 'scheduler') to use during image generation.

        seed : typing.Optional[ImageGenerationRequestSeed]
            Integer number or list of integers representing the seeds of random generators. Fixing random seed is useful when attempting to generate a specific image. Must be greater than 0 and less than 2^32.

        steps : typing.Optional[int]
            Integer repreenting how many steps of diffusion to run. Must be greater than 0 and less than or equal to 200.

        strength : typing.Optional[float]
            Floating-point number indicating how much creative the Image to Image generation mode should be. Must be greater than 0 and less than or equal to 1.0.

        style_preset : typing.Optional[SdxlStyles]
            [Not supported on SD3] Pre-defined styles used to guide the output image towards a particular style. Pre-defined styles are only supported by SDXL.

        textual_inversions : typing.Optional[typing.Dict[str, typing.Optional[str]]]
            [Not supported on SD3] A dictionary of textual inversions to be used during image generation. Textual inversions as keys and trigger words as values.

        transfer_images : typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]]
            [Not supported on SD3] A dictionary containing a mapping of trigger words to a list of sample images which demonstrate the desired object or style to transfer.

        use_refiner : typing.Optional[bool]
            [Not supported on SD3] Whether to enable and apply the SDXL refiner model to the image generation.

        vae : typing.Optional[str]
            [Not Supported on SD3] Custom VAE to be used during image generation.

        width : typing.Optional[int]
            Integer representing the width of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ImageGenerationResponse
            Successful Response

        Examples
        --------
        from octoai.client import OctoAI

        client = OctoAI(
            api_key="YOUR_API_KEY",
        )
        client.image_gen.generate_controlnet_sd15(
            prompt="prompt",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "generate/controlnet-sd15",
            base_url=self._client_wrapper.get_environment().image_gen,
            method="POST",
            json={
                "cfg_scale": cfg_scale,
                "checkpoint": checkpoint,
                "clip_skip": clip_skip,
                "controlnet": controlnet,
                "controlnet_conditioning_scale": controlnet_conditioning_scale,
                "controlnet_early_stop": controlnet_early_stop,
                "controlnet_image": controlnet_image,
                "controlnet_preprocess": controlnet_preprocess,
                "height": height,
                "high_noise_frac": high_noise_frac,
                "image_encoding": image_encoding,
                "init_image": init_image,
                "ip_adapter_image": ip_adapter_image,
                "ip_adapter_mask_image": ip_adapter_mask_image,
                "ip_adapter_method": ip_adapter_method,
                "ip_adapter_mode": ip_adapter_mode,
                "ip_adapter_scale": ip_adapter_scale,
                "loras": loras,
                "mask_image": mask_image,
                "negative_prompt": negative_prompt,
                "negative_prompt_2": negative_prompt2,
                "num_images": num_images,
                "outpainting": outpainting,
                "prompt": prompt,
                "prompt_2": prompt2,
                "sampler": sampler,
                "seed": seed,
                "steps": steps,
                "strength": strength,
                "style_preset": style_preset,
                "textual_inversions": textual_inversions,
                "transfer_images": transfer_images,
                "use_refiner": use_refiner,
                "vae": vae,
                "width": width,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(ImageGenerationResponse, _response.json())  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def generate_controlnet_sdxl(
        self,
        *,
        prompt: str,
        cfg_scale: typing.Optional[float] = OMIT,
        checkpoint: typing.Optional[str] = OMIT,
        clip_skip: typing.Optional[int] = OMIT,
        controlnet: typing.Optional[str] = OMIT,
        controlnet_conditioning_scale: typing.Optional[float] = OMIT,
        controlnet_early_stop: typing.Optional[float] = OMIT,
        controlnet_image: typing.Optional[str] = OMIT,
        controlnet_preprocess: typing.Optional[bool] = OMIT,
        height: typing.Optional[int] = OMIT,
        high_noise_frac: typing.Optional[float] = OMIT,
        image_encoding: typing.Optional[ImageEncoding] = OMIT,
        init_image: typing.Optional[str] = OMIT,
        ip_adapter_image: typing.Optional[str] = OMIT,
        ip_adapter_mask_image: typing.Optional[str] = OMIT,
        ip_adapter_method: typing.Optional[IpAdapterMethod] = OMIT,
        ip_adapter_mode: typing.Optional[IpAdapterMode] = OMIT,
        ip_adapter_scale: typing.Optional[float] = OMIT,
        loras: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        mask_image: typing.Optional[str] = OMIT,
        negative_prompt: typing.Optional[str] = OMIT,
        negative_prompt2: typing.Optional[str] = OMIT,
        num_images: typing.Optional[int] = OMIT,
        outpainting: typing.Optional[bool] = OMIT,
        prompt2: typing.Optional[str] = OMIT,
        sampler: typing.Optional[Scheduler] = OMIT,
        seed: typing.Optional[ImageGenerationRequestSeed] = OMIT,
        steps: typing.Optional[int] = OMIT,
        strength: typing.Optional[float] = OMIT,
        style_preset: typing.Optional[SdxlStyles] = OMIT,
        textual_inversions: typing.Optional[typing.Dict[str, typing.Optional[str]]] = OMIT,
        transfer_images: typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]] = OMIT,
        use_refiner: typing.Optional[bool] = OMIT,
        vae: typing.Optional[str] = OMIT,
        width: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> ImageGenerationResponse:
        """
        Generate images in response to the given request.

        Parameters
        ----------
        prompt : str
            Text describing the image content to generate.

        cfg_scale : typing.Optional[float]
            Floating-point number represeting how closely to adhere to prompt description. Must be a positive number no greater than 50.0.

        checkpoint : typing.Optional[str]
            [Not supported on SD3] Custom checkpoint to be used during image generation.

        clip_skip : typing.Optional[int]
            [Not supported on SD3] Optionally skip later layers of the text encoder. Higher values lead to more abstract interpretations of the prompt.

        controlnet : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] ControlNet to be used during image generation.

        controlnet_conditioning_scale : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] How strong the effect of the controlnet should be.

        controlnet_early_stop : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] If provided, indicates fraction of steps at which to stop applying controlnet. This can be used to sometimes generate better outputs.

        controlnet_image : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] Controlnet image encoded in b64 string for guiding image generation. Required for controlnet engines.

        controlnet_preprocess : typing.Optional[bool]
            [Not supported on SD3 or IPAdapter] Whether to apply automatic ControlNet preprocessing.

        height : typing.Optional[int]
            Integer representing the height of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        high_noise_frac : typing.Optional[float]
            [Not supported on SD3] Floating-point number that defines the fraction of steps to perform with the base model. Used only by SD XL. Must be greater than or equal to 0.0 and less than or equal to 1.0.

        image_encoding : typing.Optional[ImageEncoding]
            Define which encoding process should be applied before returning the generated image(s).

        init_image : typing.Optional[str]
            Starting point image encoded in b64 string for Image to Image generation mode.

        ip_adapter_image : typing.Optional[str]
            IP Adapter image encoded in b64 string for guiding image generation. Required for ip adapter engines.

        ip_adapter_mask_image : typing.Optional[str]
            b64 encoded mask image for ip adapter. White area should indicate where to paint.

        ip_adapter_method : typing.Optional[IpAdapterMethod]
            Define which ip adapter method should be applied during the image generation. Supported modes: full, style, and composition

        ip_adapter_mode : typing.Optional[IpAdapterMode]
            Define which ip adapter mode should be applied during the image generation. Supported modes: base

        ip_adapter_scale : typing.Optional[float]
            How strong the effect of the ip adapter should be.

        loras : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            [Not supported on SD3] A dictionary of LoRAs to apply. LoRAs as keys and their weights (float) as values.

        mask_image : typing.Optional[str]
            [Not supported on SD3] b64 encoded mask image for inpainting. White area should indicate where to paint.

        negative_prompt : typing.Optional[str]
            Text describing image traits to avoid during generation.

        negative_prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high level description of things to avoid during generation. Used only by SD XL.

        num_images : typing.Optional[int]
            Integer representing how many output images to generate with a single prompt/configuration.

        outpainting : typing.Optional[bool]
            [Not supported on SD3] Whether the request requires outpainting or not. If so, special preprocessing is applied for better results.

        prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high-level description of the image to generate. Used only by SD XL.

        sampler : typing.Optional[Scheduler]
            [Not supported on SD3] Sampler name (also known as 'scheduler') to use during image generation.

        seed : typing.Optional[ImageGenerationRequestSeed]
            Integer number or list of integers representing the seeds of random generators. Fixing random seed is useful when attempting to generate a specific image. Must be greater than 0 and less than 2^32.

        steps : typing.Optional[int]
            Integer repreenting how many steps of diffusion to run. Must be greater than 0 and less than or equal to 200.

        strength : typing.Optional[float]
            Floating-point number indicating how much creative the Image to Image generation mode should be. Must be greater than 0 and less than or equal to 1.0.

        style_preset : typing.Optional[SdxlStyles]
            [Not supported on SD3] Pre-defined styles used to guide the output image towards a particular style. Pre-defined styles are only supported by SDXL.

        textual_inversions : typing.Optional[typing.Dict[str, typing.Optional[str]]]
            [Not supported on SD3] A dictionary of textual inversions to be used during image generation. Textual inversions as keys and trigger words as values.

        transfer_images : typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]]
            [Not supported on SD3] A dictionary containing a mapping of trigger words to a list of sample images which demonstrate the desired object or style to transfer.

        use_refiner : typing.Optional[bool]
            [Not supported on SD3] Whether to enable and apply the SDXL refiner model to the image generation.

        vae : typing.Optional[str]
            [Not Supported on SD3] Custom VAE to be used during image generation.

        width : typing.Optional[int]
            Integer representing the width of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ImageGenerationResponse
            Successful Response

        Examples
        --------
        from octoai.client import OctoAI

        client = OctoAI(
            api_key="YOUR_API_KEY",
        )
        client.image_gen.generate_controlnet_sdxl(
            prompt="prompt",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "generate/controlnet-sdxl",
            base_url=self._client_wrapper.get_environment().image_gen,
            method="POST",
            json={
                "cfg_scale": cfg_scale,
                "checkpoint": checkpoint,
                "clip_skip": clip_skip,
                "controlnet": controlnet,
                "controlnet_conditioning_scale": controlnet_conditioning_scale,
                "controlnet_early_stop": controlnet_early_stop,
                "controlnet_image": controlnet_image,
                "controlnet_preprocess": controlnet_preprocess,
                "height": height,
                "high_noise_frac": high_noise_frac,
                "image_encoding": image_encoding,
                "init_image": init_image,
                "ip_adapter_image": ip_adapter_image,
                "ip_adapter_mask_image": ip_adapter_mask_image,
                "ip_adapter_method": ip_adapter_method,
                "ip_adapter_mode": ip_adapter_mode,
                "ip_adapter_scale": ip_adapter_scale,
                "loras": loras,
                "mask_image": mask_image,
                "negative_prompt": negative_prompt,
                "negative_prompt_2": negative_prompt2,
                "num_images": num_images,
                "outpainting": outpainting,
                "prompt": prompt,
                "prompt_2": prompt2,
                "sampler": sampler,
                "seed": seed,
                "steps": steps,
                "strength": strength,
                "style_preset": style_preset,
                "textual_inversions": textual_inversions,
                "transfer_images": transfer_images,
                "use_refiner": use_refiner,
                "vae": vae,
                "width": width,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(ImageGenerationResponse, _response.json())  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def ip_adapter(
        self,
        *,
        prompt: str,
        cfg_scale: typing.Optional[float] = OMIT,
        checkpoint: typing.Optional[str] = OMIT,
        clip_skip: typing.Optional[int] = OMIT,
        controlnet: typing.Optional[str] = OMIT,
        controlnet_conditioning_scale: typing.Optional[float] = OMIT,
        controlnet_early_stop: typing.Optional[float] = OMIT,
        controlnet_image: typing.Optional[str] = OMIT,
        controlnet_preprocess: typing.Optional[bool] = OMIT,
        height: typing.Optional[int] = OMIT,
        high_noise_frac: typing.Optional[float] = OMIT,
        image_encoding: typing.Optional[ImageEncoding] = OMIT,
        init_image: typing.Optional[str] = OMIT,
        ip_adapter_image: typing.Optional[str] = OMIT,
        ip_adapter_mask_image: typing.Optional[str] = OMIT,
        ip_adapter_method: typing.Optional[IpAdapterMethod] = OMIT,
        ip_adapter_mode: typing.Optional[IpAdapterMode] = OMIT,
        ip_adapter_scale: typing.Optional[float] = OMIT,
        loras: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        mask_image: typing.Optional[str] = OMIT,
        negative_prompt: typing.Optional[str] = OMIT,
        negative_prompt2: typing.Optional[str] = OMIT,
        num_images: typing.Optional[int] = OMIT,
        outpainting: typing.Optional[bool] = OMIT,
        prompt2: typing.Optional[str] = OMIT,
        sampler: typing.Optional[Scheduler] = OMIT,
        seed: typing.Optional[ImageGenerationRequestSeed] = OMIT,
        steps: typing.Optional[int] = OMIT,
        strength: typing.Optional[float] = OMIT,
        style_preset: typing.Optional[SdxlStyles] = OMIT,
        textual_inversions: typing.Optional[typing.Dict[str, typing.Optional[str]]] = OMIT,
        transfer_images: typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]] = OMIT,
        use_refiner: typing.Optional[bool] = OMIT,
        vae: typing.Optional[str] = OMIT,
        width: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> ImageGenerationResponse:
        """
        Generate images in response to the given request.

        Parameters
        ----------
        prompt : str
            Text describing the image content to generate.

        cfg_scale : typing.Optional[float]
            Floating-point number represeting how closely to adhere to prompt description. Must be a positive number no greater than 50.0.

        checkpoint : typing.Optional[str]
            [Not supported on SD3] Custom checkpoint to be used during image generation.

        clip_skip : typing.Optional[int]
            [Not supported on SD3] Optionally skip later layers of the text encoder. Higher values lead to more abstract interpretations of the prompt.

        controlnet : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] ControlNet to be used during image generation.

        controlnet_conditioning_scale : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] How strong the effect of the controlnet should be.

        controlnet_early_stop : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] If provided, indicates fraction of steps at which to stop applying controlnet. This can be used to sometimes generate better outputs.

        controlnet_image : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] Controlnet image encoded in b64 string for guiding image generation. Required for controlnet engines.

        controlnet_preprocess : typing.Optional[bool]
            [Not supported on SD3 or IPAdapter] Whether to apply automatic ControlNet preprocessing.

        height : typing.Optional[int]
            Integer representing the height of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        high_noise_frac : typing.Optional[float]
            [Not supported on SD3] Floating-point number that defines the fraction of steps to perform with the base model. Used only by SD XL. Must be greater than or equal to 0.0 and less than or equal to 1.0.

        image_encoding : typing.Optional[ImageEncoding]
            Define which encoding process should be applied before returning the generated image(s).

        init_image : typing.Optional[str]
            Starting point image encoded in b64 string for Image to Image generation mode.

        ip_adapter_image : typing.Optional[str]
            IP Adapter image encoded in b64 string for guiding image generation. Required for ip adapter engines.

        ip_adapter_mask_image : typing.Optional[str]
            b64 encoded mask image for ip adapter. White area should indicate where to paint.

        ip_adapter_method : typing.Optional[IpAdapterMethod]
            Define which ip adapter method should be applied during the image generation. Supported modes: full, style, and composition

        ip_adapter_mode : typing.Optional[IpAdapterMode]
            Define which ip adapter mode should be applied during the image generation. Supported modes: base

        ip_adapter_scale : typing.Optional[float]
            How strong the effect of the ip adapter should be.

        loras : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            [Not supported on SD3] A dictionary of LoRAs to apply. LoRAs as keys and their weights (float) as values.

        mask_image : typing.Optional[str]
            [Not supported on SD3] b64 encoded mask image for inpainting. White area should indicate where to paint.

        negative_prompt : typing.Optional[str]
            Text describing image traits to avoid during generation.

        negative_prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high level description of things to avoid during generation. Used only by SD XL.

        num_images : typing.Optional[int]
            Integer representing how many output images to generate with a single prompt/configuration.

        outpainting : typing.Optional[bool]
            [Not supported on SD3] Whether the request requires outpainting or not. If so, special preprocessing is applied for better results.

        prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high-level description of the image to generate. Used only by SD XL.

        sampler : typing.Optional[Scheduler]
            [Not supported on SD3] Sampler name (also known as 'scheduler') to use during image generation.

        seed : typing.Optional[ImageGenerationRequestSeed]
            Integer number or list of integers representing the seeds of random generators. Fixing random seed is useful when attempting to generate a specific image. Must be greater than 0 and less than 2^32.

        steps : typing.Optional[int]
            Integer repreenting how many steps of diffusion to run. Must be greater than 0 and less than or equal to 200.

        strength : typing.Optional[float]
            Floating-point number indicating how much creative the Image to Image generation mode should be. Must be greater than 0 and less than or equal to 1.0.

        style_preset : typing.Optional[SdxlStyles]
            [Not supported on SD3] Pre-defined styles used to guide the output image towards a particular style. Pre-defined styles are only supported by SDXL.

        textual_inversions : typing.Optional[typing.Dict[str, typing.Optional[str]]]
            [Not supported on SD3] A dictionary of textual inversions to be used during image generation. Textual inversions as keys and trigger words as values.

        transfer_images : typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]]
            [Not supported on SD3] A dictionary containing a mapping of trigger words to a list of sample images which demonstrate the desired object or style to transfer.

        use_refiner : typing.Optional[bool]
            [Not supported on SD3] Whether to enable and apply the SDXL refiner model to the image generation.

        vae : typing.Optional[str]
            [Not Supported on SD3] Custom VAE to be used during image generation.

        width : typing.Optional[int]
            Integer representing the width of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ImageGenerationResponse
            Successful Response

        Examples
        --------
        from octoai.client import OctoAI

        client = OctoAI(
            api_key="YOUR_API_KEY",
        )
        client.image_gen.ip_adapter(
            prompt="prompt",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "generate/ip-adapter-sdxl",
            base_url=self._client_wrapper.get_environment().image_gen,
            method="POST",
            json={
                "cfg_scale": cfg_scale,
                "checkpoint": checkpoint,
                "clip_skip": clip_skip,
                "controlnet": controlnet,
                "controlnet_conditioning_scale": controlnet_conditioning_scale,
                "controlnet_early_stop": controlnet_early_stop,
                "controlnet_image": controlnet_image,
                "controlnet_preprocess": controlnet_preprocess,
                "height": height,
                "high_noise_frac": high_noise_frac,
                "image_encoding": image_encoding,
                "init_image": init_image,
                "ip_adapter_image": ip_adapter_image,
                "ip_adapter_mask_image": ip_adapter_mask_image,
                "ip_adapter_method": ip_adapter_method,
                "ip_adapter_mode": ip_adapter_mode,
                "ip_adapter_scale": ip_adapter_scale,
                "loras": loras,
                "mask_image": mask_image,
                "negative_prompt": negative_prompt,
                "negative_prompt_2": negative_prompt2,
                "num_images": num_images,
                "outpainting": outpainting,
                "prompt": prompt,
                "prompt_2": prompt2,
                "sampler": sampler,
                "seed": seed,
                "steps": steps,
                "strength": strength,
                "style_preset": style_preset,
                "textual_inversions": textual_inversions,
                "transfer_images": transfer_images,
                "use_refiner": use_refiner,
                "vae": vae,
                "width": width,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(ImageGenerationResponse, _response.json())  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def generate_sd(
        self,
        *,
        prompt: str,
        cfg_scale: typing.Optional[float] = OMIT,
        checkpoint: typing.Optional[str] = OMIT,
        clip_skip: typing.Optional[int] = OMIT,
        controlnet: typing.Optional[str] = OMIT,
        controlnet_conditioning_scale: typing.Optional[float] = OMIT,
        controlnet_early_stop: typing.Optional[float] = OMIT,
        controlnet_image: typing.Optional[str] = OMIT,
        controlnet_preprocess: typing.Optional[bool] = OMIT,
        height: typing.Optional[int] = OMIT,
        high_noise_frac: typing.Optional[float] = OMIT,
        image_encoding: typing.Optional[ImageEncoding] = OMIT,
        init_image: typing.Optional[str] = OMIT,
        ip_adapter_image: typing.Optional[str] = OMIT,
        ip_adapter_mask_image: typing.Optional[str] = OMIT,
        ip_adapter_method: typing.Optional[IpAdapterMethod] = OMIT,
        ip_adapter_mode: typing.Optional[IpAdapterMode] = OMIT,
        ip_adapter_scale: typing.Optional[float] = OMIT,
        loras: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        mask_image: typing.Optional[str] = OMIT,
        negative_prompt: typing.Optional[str] = OMIT,
        negative_prompt2: typing.Optional[str] = OMIT,
        num_images: typing.Optional[int] = OMIT,
        outpainting: typing.Optional[bool] = OMIT,
        prompt2: typing.Optional[str] = OMIT,
        sampler: typing.Optional[Scheduler] = OMIT,
        seed: typing.Optional[ImageGenerationRequestSeed] = OMIT,
        steps: typing.Optional[int] = OMIT,
        strength: typing.Optional[float] = OMIT,
        style_preset: typing.Optional[SdxlStyles] = OMIT,
        textual_inversions: typing.Optional[typing.Dict[str, typing.Optional[str]]] = OMIT,
        transfer_images: typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]] = OMIT,
        use_refiner: typing.Optional[bool] = OMIT,
        vae: typing.Optional[str] = OMIT,
        width: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> ImageGenerationResponse:
        """
        Generate images in response to the given request.

        Parameters
        ----------
        prompt : str
            Text describing the image content to generate.

        cfg_scale : typing.Optional[float]
            Floating-point number represeting how closely to adhere to prompt description. Must be a positive number no greater than 50.0.

        checkpoint : typing.Optional[str]
            [Not supported on SD3] Custom checkpoint to be used during image generation.

        clip_skip : typing.Optional[int]
            [Not supported on SD3] Optionally skip later layers of the text encoder. Higher values lead to more abstract interpretations of the prompt.

        controlnet : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] ControlNet to be used during image generation.

        controlnet_conditioning_scale : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] How strong the effect of the controlnet should be.

        controlnet_early_stop : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] If provided, indicates fraction of steps at which to stop applying controlnet. This can be used to sometimes generate better outputs.

        controlnet_image : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] Controlnet image encoded in b64 string for guiding image generation. Required for controlnet engines.

        controlnet_preprocess : typing.Optional[bool]
            [Not supported on SD3 or IPAdapter] Whether to apply automatic ControlNet preprocessing.

        height : typing.Optional[int]
            Integer representing the height of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        high_noise_frac : typing.Optional[float]
            [Not supported on SD3] Floating-point number that defines the fraction of steps to perform with the base model. Used only by SD XL. Must be greater than or equal to 0.0 and less than or equal to 1.0.

        image_encoding : typing.Optional[ImageEncoding]
            Define which encoding process should be applied before returning the generated image(s).

        init_image : typing.Optional[str]
            Starting point image encoded in b64 string for Image to Image generation mode.

        ip_adapter_image : typing.Optional[str]
            IP Adapter image encoded in b64 string for guiding image generation. Required for ip adapter engines.

        ip_adapter_mask_image : typing.Optional[str]
            b64 encoded mask image for ip adapter. White area should indicate where to paint.

        ip_adapter_method : typing.Optional[IpAdapterMethod]
            Define which ip adapter method should be applied during the image generation. Supported modes: full, style, and composition

        ip_adapter_mode : typing.Optional[IpAdapterMode]
            Define which ip adapter mode should be applied during the image generation. Supported modes: base

        ip_adapter_scale : typing.Optional[float]
            How strong the effect of the ip adapter should be.

        loras : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            [Not supported on SD3] A dictionary of LoRAs to apply. LoRAs as keys and their weights (float) as values.

        mask_image : typing.Optional[str]
            [Not supported on SD3] b64 encoded mask image for inpainting. White area should indicate where to paint.

        negative_prompt : typing.Optional[str]
            Text describing image traits to avoid during generation.

        negative_prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high level description of things to avoid during generation. Used only by SD XL.

        num_images : typing.Optional[int]
            Integer representing how many output images to generate with a single prompt/configuration.

        outpainting : typing.Optional[bool]
            [Not supported on SD3] Whether the request requires outpainting or not. If so, special preprocessing is applied for better results.

        prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high-level description of the image to generate. Used only by SD XL.

        sampler : typing.Optional[Scheduler]
            [Not supported on SD3] Sampler name (also known as 'scheduler') to use during image generation.

        seed : typing.Optional[ImageGenerationRequestSeed]
            Integer number or list of integers representing the seeds of random generators. Fixing random seed is useful when attempting to generate a specific image. Must be greater than 0 and less than 2^32.

        steps : typing.Optional[int]
            Integer repreenting how many steps of diffusion to run. Must be greater than 0 and less than or equal to 200.

        strength : typing.Optional[float]
            Floating-point number indicating how much creative the Image to Image generation mode should be. Must be greater than 0 and less than or equal to 1.0.

        style_preset : typing.Optional[SdxlStyles]
            [Not supported on SD3] Pre-defined styles used to guide the output image towards a particular style. Pre-defined styles are only supported by SDXL.

        textual_inversions : typing.Optional[typing.Dict[str, typing.Optional[str]]]
            [Not supported on SD3] A dictionary of textual inversions to be used during image generation. Textual inversions as keys and trigger words as values.

        transfer_images : typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]]
            [Not supported on SD3] A dictionary containing a mapping of trigger words to a list of sample images which demonstrate the desired object or style to transfer.

        use_refiner : typing.Optional[bool]
            [Not supported on SD3] Whether to enable and apply the SDXL refiner model to the image generation.

        vae : typing.Optional[str]
            [Not Supported on SD3] Custom VAE to be used during image generation.

        width : typing.Optional[int]
            Integer representing the width of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ImageGenerationResponse
            Successful Response

        Examples
        --------
        from octoai.client import OctoAI

        client = OctoAI(
            api_key="YOUR_API_KEY",
        )
        client.image_gen.generate_sd(
            prompt="prompt",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "generate/sd",
            base_url=self._client_wrapper.get_environment().image_gen,
            method="POST",
            json={
                "cfg_scale": cfg_scale,
                "checkpoint": checkpoint,
                "clip_skip": clip_skip,
                "controlnet": controlnet,
                "controlnet_conditioning_scale": controlnet_conditioning_scale,
                "controlnet_early_stop": controlnet_early_stop,
                "controlnet_image": controlnet_image,
                "controlnet_preprocess": controlnet_preprocess,
                "height": height,
                "high_noise_frac": high_noise_frac,
                "image_encoding": image_encoding,
                "init_image": init_image,
                "ip_adapter_image": ip_adapter_image,
                "ip_adapter_mask_image": ip_adapter_mask_image,
                "ip_adapter_method": ip_adapter_method,
                "ip_adapter_mode": ip_adapter_mode,
                "ip_adapter_scale": ip_adapter_scale,
                "loras": loras,
                "mask_image": mask_image,
                "negative_prompt": negative_prompt,
                "negative_prompt_2": negative_prompt2,
                "num_images": num_images,
                "outpainting": outpainting,
                "prompt": prompt,
                "prompt_2": prompt2,
                "sampler": sampler,
                "seed": seed,
                "steps": steps,
                "strength": strength,
                "style_preset": style_preset,
                "textual_inversions": textual_inversions,
                "transfer_images": transfer_images,
                "use_refiner": use_refiner,
                "vae": vae,
                "width": width,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(ImageGenerationResponse, _response.json())  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def generate_sd3(
        self,
        *,
        prompt: str,
        cfg_scale: typing.Optional[float] = OMIT,
        checkpoint: typing.Optional[str] = OMIT,
        clip_skip: typing.Optional[int] = OMIT,
        controlnet: typing.Optional[str] = OMIT,
        controlnet_conditioning_scale: typing.Optional[float] = OMIT,
        controlnet_early_stop: typing.Optional[float] = OMIT,
        controlnet_image: typing.Optional[str] = OMIT,
        controlnet_preprocess: typing.Optional[bool] = OMIT,
        height: typing.Optional[int] = OMIT,
        high_noise_frac: typing.Optional[float] = OMIT,
        image_encoding: typing.Optional[ImageEncoding] = OMIT,
        init_image: typing.Optional[str] = OMIT,
        ip_adapter_image: typing.Optional[str] = OMIT,
        ip_adapter_mask_image: typing.Optional[str] = OMIT,
        ip_adapter_method: typing.Optional[IpAdapterMethod] = OMIT,
        ip_adapter_mode: typing.Optional[IpAdapterMode] = OMIT,
        ip_adapter_scale: typing.Optional[float] = OMIT,
        loras: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        mask_image: typing.Optional[str] = OMIT,
        negative_prompt: typing.Optional[str] = OMIT,
        negative_prompt2: typing.Optional[str] = OMIT,
        num_images: typing.Optional[int] = OMIT,
        outpainting: typing.Optional[bool] = OMIT,
        prompt2: typing.Optional[str] = OMIT,
        sampler: typing.Optional[Scheduler] = OMIT,
        seed: typing.Optional[ImageGenerationRequestSeed] = OMIT,
        steps: typing.Optional[int] = OMIT,
        strength: typing.Optional[float] = OMIT,
        style_preset: typing.Optional[SdxlStyles] = OMIT,
        textual_inversions: typing.Optional[typing.Dict[str, typing.Optional[str]]] = OMIT,
        transfer_images: typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]] = OMIT,
        use_refiner: typing.Optional[bool] = OMIT,
        vae: typing.Optional[str] = OMIT,
        width: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> ImageGenerationResponse:
        """
        Generate images in response to the given request.

        Parameters
        ----------
        prompt : str
            Text describing the image content to generate.

        cfg_scale : typing.Optional[float]
            Floating-point number represeting how closely to adhere to prompt description. Must be a positive number no greater than 50.0.

        checkpoint : typing.Optional[str]
            [Not supported on SD3] Custom checkpoint to be used during image generation.

        clip_skip : typing.Optional[int]
            [Not supported on SD3] Optionally skip later layers of the text encoder. Higher values lead to more abstract interpretations of the prompt.

        controlnet : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] ControlNet to be used during image generation.

        controlnet_conditioning_scale : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] How strong the effect of the controlnet should be.

        controlnet_early_stop : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] If provided, indicates fraction of steps at which to stop applying controlnet. This can be used to sometimes generate better outputs.

        controlnet_image : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] Controlnet image encoded in b64 string for guiding image generation. Required for controlnet engines.

        controlnet_preprocess : typing.Optional[bool]
            [Not supported on SD3 or IPAdapter] Whether to apply automatic ControlNet preprocessing.

        height : typing.Optional[int]
            Integer representing the height of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        high_noise_frac : typing.Optional[float]
            [Not supported on SD3] Floating-point number that defines the fraction of steps to perform with the base model. Used only by SD XL. Must be greater than or equal to 0.0 and less than or equal to 1.0.

        image_encoding : typing.Optional[ImageEncoding]
            Define which encoding process should be applied before returning the generated image(s).

        init_image : typing.Optional[str]
            Starting point image encoded in b64 string for Image to Image generation mode.

        ip_adapter_image : typing.Optional[str]
            IP Adapter image encoded in b64 string for guiding image generation. Required for ip adapter engines.

        ip_adapter_mask_image : typing.Optional[str]
            b64 encoded mask image for ip adapter. White area should indicate where to paint.

        ip_adapter_method : typing.Optional[IpAdapterMethod]
            Define which ip adapter method should be applied during the image generation. Supported modes: full, style, and composition

        ip_adapter_mode : typing.Optional[IpAdapterMode]
            Define which ip adapter mode should be applied during the image generation. Supported modes: base

        ip_adapter_scale : typing.Optional[float]
            How strong the effect of the ip adapter should be.

        loras : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            [Not supported on SD3] A dictionary of LoRAs to apply. LoRAs as keys and their weights (float) as values.

        mask_image : typing.Optional[str]
            [Not supported on SD3] b64 encoded mask image for inpainting. White area should indicate where to paint.

        negative_prompt : typing.Optional[str]
            Text describing image traits to avoid during generation.

        negative_prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high level description of things to avoid during generation. Used only by SD XL.

        num_images : typing.Optional[int]
            Integer representing how many output images to generate with a single prompt/configuration.

        outpainting : typing.Optional[bool]
            [Not supported on SD3] Whether the request requires outpainting or not. If so, special preprocessing is applied for better results.

        prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high-level description of the image to generate. Used only by SD XL.

        sampler : typing.Optional[Scheduler]
            [Not supported on SD3] Sampler name (also known as 'scheduler') to use during image generation.

        seed : typing.Optional[ImageGenerationRequestSeed]
            Integer number or list of integers representing the seeds of random generators. Fixing random seed is useful when attempting to generate a specific image. Must be greater than 0 and less than 2^32.

        steps : typing.Optional[int]
            Integer repreenting how many steps of diffusion to run. Must be greater than 0 and less than or equal to 200.

        strength : typing.Optional[float]
            Floating-point number indicating how much creative the Image to Image generation mode should be. Must be greater than 0 and less than or equal to 1.0.

        style_preset : typing.Optional[SdxlStyles]
            [Not supported on SD3] Pre-defined styles used to guide the output image towards a particular style. Pre-defined styles are only supported by SDXL.

        textual_inversions : typing.Optional[typing.Dict[str, typing.Optional[str]]]
            [Not supported on SD3] A dictionary of textual inversions to be used during image generation. Textual inversions as keys and trigger words as values.

        transfer_images : typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]]
            [Not supported on SD3] A dictionary containing a mapping of trigger words to a list of sample images which demonstrate the desired object or style to transfer.

        use_refiner : typing.Optional[bool]
            [Not supported on SD3] Whether to enable and apply the SDXL refiner model to the image generation.

        vae : typing.Optional[str]
            [Not Supported on SD3] Custom VAE to be used during image generation.

        width : typing.Optional[int]
            Integer representing the width of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ImageGenerationResponse
            Successful Response

        Examples
        --------
        from octoai.client import OctoAI

        client = OctoAI(
            api_key="YOUR_API_KEY",
        )
        client.image_gen.generate_sd3(
            prompt="prompt",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "generate/sd3",
            base_url=self._client_wrapper.get_environment().image_gen,
            method="POST",
            json={
                "cfg_scale": cfg_scale,
                "checkpoint": checkpoint,
                "clip_skip": clip_skip,
                "controlnet": controlnet,
                "controlnet_conditioning_scale": controlnet_conditioning_scale,
                "controlnet_early_stop": controlnet_early_stop,
                "controlnet_image": controlnet_image,
                "controlnet_preprocess": controlnet_preprocess,
                "height": height,
                "high_noise_frac": high_noise_frac,
                "image_encoding": image_encoding,
                "init_image": init_image,
                "ip_adapter_image": ip_adapter_image,
                "ip_adapter_mask_image": ip_adapter_mask_image,
                "ip_adapter_method": ip_adapter_method,
                "ip_adapter_mode": ip_adapter_mode,
                "ip_adapter_scale": ip_adapter_scale,
                "loras": loras,
                "mask_image": mask_image,
                "negative_prompt": negative_prompt,
                "negative_prompt_2": negative_prompt2,
                "num_images": num_images,
                "outpainting": outpainting,
                "prompt": prompt,
                "prompt_2": prompt2,
                "sampler": sampler,
                "seed": seed,
                "steps": steps,
                "strength": strength,
                "style_preset": style_preset,
                "textual_inversions": textual_inversions,
                "transfer_images": transfer_images,
                "use_refiner": use_refiner,
                "vae": vae,
                "width": width,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(ImageGenerationResponse, _response.json())  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def generate_sdxl(
        self,
        *,
        prompt: str,
        cfg_scale: typing.Optional[float] = OMIT,
        checkpoint: typing.Optional[str] = OMIT,
        clip_skip: typing.Optional[int] = OMIT,
        controlnet: typing.Optional[str] = OMIT,
        controlnet_conditioning_scale: typing.Optional[float] = OMIT,
        controlnet_early_stop: typing.Optional[float] = OMIT,
        controlnet_image: typing.Optional[str] = OMIT,
        controlnet_preprocess: typing.Optional[bool] = OMIT,
        height: typing.Optional[int] = OMIT,
        high_noise_frac: typing.Optional[float] = OMIT,
        image_encoding: typing.Optional[ImageEncoding] = OMIT,
        init_image: typing.Optional[str] = OMIT,
        ip_adapter_image: typing.Optional[str] = OMIT,
        ip_adapter_mask_image: typing.Optional[str] = OMIT,
        ip_adapter_method: typing.Optional[IpAdapterMethod] = OMIT,
        ip_adapter_mode: typing.Optional[IpAdapterMode] = OMIT,
        ip_adapter_scale: typing.Optional[float] = OMIT,
        loras: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        mask_image: typing.Optional[str] = OMIT,
        negative_prompt: typing.Optional[str] = OMIT,
        negative_prompt2: typing.Optional[str] = OMIT,
        num_images: typing.Optional[int] = OMIT,
        outpainting: typing.Optional[bool] = OMIT,
        prompt2: typing.Optional[str] = OMIT,
        sampler: typing.Optional[Scheduler] = OMIT,
        seed: typing.Optional[ImageGenerationRequestSeed] = OMIT,
        steps: typing.Optional[int] = OMIT,
        strength: typing.Optional[float] = OMIT,
        style_preset: typing.Optional[SdxlStyles] = OMIT,
        textual_inversions: typing.Optional[typing.Dict[str, typing.Optional[str]]] = OMIT,
        transfer_images: typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]] = OMIT,
        use_refiner: typing.Optional[bool] = OMIT,
        vae: typing.Optional[str] = OMIT,
        width: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> ImageGenerationResponse:
        """
        Generate images in response to the given request.

        Parameters
        ----------
        prompt : str
            Text describing the image content to generate.

        cfg_scale : typing.Optional[float]
            Floating-point number represeting how closely to adhere to prompt description. Must be a positive number no greater than 50.0.

        checkpoint : typing.Optional[str]
            [Not supported on SD3] Custom checkpoint to be used during image generation.

        clip_skip : typing.Optional[int]
            [Not supported on SD3] Optionally skip later layers of the text encoder. Higher values lead to more abstract interpretations of the prompt.

        controlnet : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] ControlNet to be used during image generation.

        controlnet_conditioning_scale : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] How strong the effect of the controlnet should be.

        controlnet_early_stop : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] If provided, indicates fraction of steps at which to stop applying controlnet. This can be used to sometimes generate better outputs.

        controlnet_image : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] Controlnet image encoded in b64 string for guiding image generation. Required for controlnet engines.

        controlnet_preprocess : typing.Optional[bool]
            [Not supported on SD3 or IPAdapter] Whether to apply automatic ControlNet preprocessing.

        height : typing.Optional[int]
            Integer representing the height of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        high_noise_frac : typing.Optional[float]
            [Not supported on SD3] Floating-point number that defines the fraction of steps to perform with the base model. Used only by SD XL. Must be greater than or equal to 0.0 and less than or equal to 1.0.

        image_encoding : typing.Optional[ImageEncoding]
            Define which encoding process should be applied before returning the generated image(s).

        init_image : typing.Optional[str]
            Starting point image encoded in b64 string for Image to Image generation mode.

        ip_adapter_image : typing.Optional[str]
            IP Adapter image encoded in b64 string for guiding image generation. Required for ip adapter engines.

        ip_adapter_mask_image : typing.Optional[str]
            b64 encoded mask image for ip adapter. White area should indicate where to paint.

        ip_adapter_method : typing.Optional[IpAdapterMethod]
            Define which ip adapter method should be applied during the image generation. Supported modes: full, style, and composition

        ip_adapter_mode : typing.Optional[IpAdapterMode]
            Define which ip adapter mode should be applied during the image generation. Supported modes: base

        ip_adapter_scale : typing.Optional[float]
            How strong the effect of the ip adapter should be.

        loras : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            [Not supported on SD3] A dictionary of LoRAs to apply. LoRAs as keys and their weights (float) as values.

        mask_image : typing.Optional[str]
            [Not supported on SD3] b64 encoded mask image for inpainting. White area should indicate where to paint.

        negative_prompt : typing.Optional[str]
            Text describing image traits to avoid during generation.

        negative_prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high level description of things to avoid during generation. Used only by SD XL.

        num_images : typing.Optional[int]
            Integer representing how many output images to generate with a single prompt/configuration.

        outpainting : typing.Optional[bool]
            [Not supported on SD3] Whether the request requires outpainting or not. If so, special preprocessing is applied for better results.

        prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high-level description of the image to generate. Used only by SD XL.

        sampler : typing.Optional[Scheduler]
            [Not supported on SD3] Sampler name (also known as 'scheduler') to use during image generation.

        seed : typing.Optional[ImageGenerationRequestSeed]
            Integer number or list of integers representing the seeds of random generators. Fixing random seed is useful when attempting to generate a specific image. Must be greater than 0 and less than 2^32.

        steps : typing.Optional[int]
            Integer repreenting how many steps of diffusion to run. Must be greater than 0 and less than or equal to 200.

        strength : typing.Optional[float]
            Floating-point number indicating how much creative the Image to Image generation mode should be. Must be greater than 0 and less than or equal to 1.0.

        style_preset : typing.Optional[SdxlStyles]
            [Not supported on SD3] Pre-defined styles used to guide the output image towards a particular style. Pre-defined styles are only supported by SDXL.

        textual_inversions : typing.Optional[typing.Dict[str, typing.Optional[str]]]
            [Not supported on SD3] A dictionary of textual inversions to be used during image generation. Textual inversions as keys and trigger words as values.

        transfer_images : typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]]
            [Not supported on SD3] A dictionary containing a mapping of trigger words to a list of sample images which demonstrate the desired object or style to transfer.

        use_refiner : typing.Optional[bool]
            [Not supported on SD3] Whether to enable and apply the SDXL refiner model to the image generation.

        vae : typing.Optional[str]
            [Not Supported on SD3] Custom VAE to be used during image generation.

        width : typing.Optional[int]
            Integer representing the width of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ImageGenerationResponse
            Successful Response

        Examples
        --------
        from octoai.client import OctoAI

        client = OctoAI(
            api_key="YOUR_API_KEY",
        )
        client.image_gen.generate_sdxl(
            prompt="prompt",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "generate/sdxl",
            base_url=self._client_wrapper.get_environment().image_gen,
            method="POST",
            json={
                "cfg_scale": cfg_scale,
                "checkpoint": checkpoint,
                "clip_skip": clip_skip,
                "controlnet": controlnet,
                "controlnet_conditioning_scale": controlnet_conditioning_scale,
                "controlnet_early_stop": controlnet_early_stop,
                "controlnet_image": controlnet_image,
                "controlnet_preprocess": controlnet_preprocess,
                "height": height,
                "high_noise_frac": high_noise_frac,
                "image_encoding": image_encoding,
                "init_image": init_image,
                "ip_adapter_image": ip_adapter_image,
                "ip_adapter_mask_image": ip_adapter_mask_image,
                "ip_adapter_method": ip_adapter_method,
                "ip_adapter_mode": ip_adapter_mode,
                "ip_adapter_scale": ip_adapter_scale,
                "loras": loras,
                "mask_image": mask_image,
                "negative_prompt": negative_prompt,
                "negative_prompt_2": negative_prompt2,
                "num_images": num_images,
                "outpainting": outpainting,
                "prompt": prompt,
                "prompt_2": prompt2,
                "sampler": sampler,
                "seed": seed,
                "steps": steps,
                "strength": strength,
                "style_preset": style_preset,
                "textual_inversions": textual_inversions,
                "transfer_images": transfer_images,
                "use_refiner": use_refiner,
                "vae": vae,
                "width": width,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(ImageGenerationResponse, _response.json())  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def generate_svd(
        self,
        *,
        image: str,
        cfg_scale: typing.Optional[float] = OMIT,
        constant_rate_factor: typing.Optional[int] = OMIT,
        fps: typing.Optional[int] = OMIT,
        height: typing.Optional[int] = OMIT,
        motion_scale: typing.Optional[float] = OMIT,
        noise_aug_strength: typing.Optional[float] = OMIT,
        num_videos: typing.Optional[int] = OMIT,
        seed: typing.Optional[VideoGenerationRequestSeed] = OMIT,
        steps: typing.Optional[int] = OMIT,
        width: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> VideoGenerationResponse:
        """
        Generate videos in response to the given request.

        Parameters
        ----------
        image : str
            Starting point image encoded in base64 string.

        cfg_scale : typing.Optional[float]
            Floating-point number represeting how closely to adhere to 'image'. Must be a positive number no greater than 10.0.

        constant_rate_factor : typing.Optional[int]
            Integer representing the quality of the video encoding. Higher quality means the file will be larger.The range of the CRF scale is [0,51], where 0 is lossless, 23 is the default, and 51 is worst quality possible.

        fps : typing.Optional[int]
            Integer representing how fast the generated frames should play back.

        height : typing.Optional[int]
            Integer representing the height of video/animation to generate.If not provided, the output height will be inferred from the input 'image', and the closest resolution supported will be chosen. Supported resolutions (w,h): {(576, 1024), (1024, 576), (768, 768)}.

        motion_scale : typing.Optional[float]
            A floating point number between 0.0 and 1.0 indicating how much motion should be in the generated video/animation.

        noise_aug_strength : typing.Optional[float]
            A floating point number between 0.0 and 1.0 indicatiing how much noise to add to the initial image. Higher values encourage creativity.

        num_videos : typing.Optional[int]
            Integer representing how many output videos/animations to generate with a single 'image' and configuration.

        seed : typing.Optional[VideoGenerationRequestSeed]
            Integer number or list of integers representing the seeds of random generators.Fixing random seed is useful when attempting to generate a specific video/animation (or set of videos/animations). Must be greater than 0 and less than 2^32.

        steps : typing.Optional[int]
            Integer repreenting how many steps of diffusion to run. Must be greater than 0 and less than or equal to 50.

        width : typing.Optional[int]
            Integer representing the width of video/animation to generate.If not provided, the output width will be inferred from the input 'image', and the closest resolution supported will be chosen. Supported resolutions (w,h): {(576, 1024), (1024, 576), (768, 768)}.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        VideoGenerationResponse
            Successful Response

        Examples
        --------
        from octoai.client import OctoAI

        client = OctoAI(
            api_key="YOUR_API_KEY",
        )
        client.image_gen.generate_svd(
            image="image",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "generate/svd",
            base_url=self._client_wrapper.get_environment().image_gen,
            method="POST",
            json={
                "cfg_scale": cfg_scale,
                "constant_rate_factor": constant_rate_factor,
                "fps": fps,
                "height": height,
                "image": image,
                "motion_scale": motion_scale,
                "noise_aug_strength": noise_aug_strength,
                "num_videos": num_videos,
                "seed": seed,
                "steps": steps,
                "width": width,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(VideoGenerationResponse, _response.json())  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncImageGenClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def generate_controlnet_sd15(
        self,
        *,
        prompt: str,
        cfg_scale: typing.Optional[float] = OMIT,
        checkpoint: typing.Optional[str] = OMIT,
        clip_skip: typing.Optional[int] = OMIT,
        controlnet: typing.Optional[str] = OMIT,
        controlnet_conditioning_scale: typing.Optional[float] = OMIT,
        controlnet_early_stop: typing.Optional[float] = OMIT,
        controlnet_image: typing.Optional[str] = OMIT,
        controlnet_preprocess: typing.Optional[bool] = OMIT,
        height: typing.Optional[int] = OMIT,
        high_noise_frac: typing.Optional[float] = OMIT,
        image_encoding: typing.Optional[ImageEncoding] = OMIT,
        init_image: typing.Optional[str] = OMIT,
        ip_adapter_image: typing.Optional[str] = OMIT,
        ip_adapter_mask_image: typing.Optional[str] = OMIT,
        ip_adapter_method: typing.Optional[IpAdapterMethod] = OMIT,
        ip_adapter_mode: typing.Optional[IpAdapterMode] = OMIT,
        ip_adapter_scale: typing.Optional[float] = OMIT,
        loras: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        mask_image: typing.Optional[str] = OMIT,
        negative_prompt: typing.Optional[str] = OMIT,
        negative_prompt2: typing.Optional[str] = OMIT,
        num_images: typing.Optional[int] = OMIT,
        outpainting: typing.Optional[bool] = OMIT,
        prompt2: typing.Optional[str] = OMIT,
        sampler: typing.Optional[Scheduler] = OMIT,
        seed: typing.Optional[ImageGenerationRequestSeed] = OMIT,
        steps: typing.Optional[int] = OMIT,
        strength: typing.Optional[float] = OMIT,
        style_preset: typing.Optional[SdxlStyles] = OMIT,
        textual_inversions: typing.Optional[typing.Dict[str, typing.Optional[str]]] = OMIT,
        transfer_images: typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]] = OMIT,
        use_refiner: typing.Optional[bool] = OMIT,
        vae: typing.Optional[str] = OMIT,
        width: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> ImageGenerationResponse:
        """
        Generate images in response to the given request.

        Parameters
        ----------
        prompt : str
            Text describing the image content to generate.

        cfg_scale : typing.Optional[float]
            Floating-point number represeting how closely to adhere to prompt description. Must be a positive number no greater than 50.0.

        checkpoint : typing.Optional[str]
            [Not supported on SD3] Custom checkpoint to be used during image generation.

        clip_skip : typing.Optional[int]
            [Not supported on SD3] Optionally skip later layers of the text encoder. Higher values lead to more abstract interpretations of the prompt.

        controlnet : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] ControlNet to be used during image generation.

        controlnet_conditioning_scale : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] How strong the effect of the controlnet should be.

        controlnet_early_stop : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] If provided, indicates fraction of steps at which to stop applying controlnet. This can be used to sometimes generate better outputs.

        controlnet_image : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] Controlnet image encoded in b64 string for guiding image generation. Required for controlnet engines.

        controlnet_preprocess : typing.Optional[bool]
            [Not supported on SD3 or IPAdapter] Whether to apply automatic ControlNet preprocessing.

        height : typing.Optional[int]
            Integer representing the height of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        high_noise_frac : typing.Optional[float]
            [Not supported on SD3] Floating-point number that defines the fraction of steps to perform with the base model. Used only by SD XL. Must be greater than or equal to 0.0 and less than or equal to 1.0.

        image_encoding : typing.Optional[ImageEncoding]
            Define which encoding process should be applied before returning the generated image(s).

        init_image : typing.Optional[str]
            Starting point image encoded in b64 string for Image to Image generation mode.

        ip_adapter_image : typing.Optional[str]
            IP Adapter image encoded in b64 string for guiding image generation. Required for ip adapter engines.

        ip_adapter_mask_image : typing.Optional[str]
            b64 encoded mask image for ip adapter. White area should indicate where to paint.

        ip_adapter_method : typing.Optional[IpAdapterMethod]
            Define which ip adapter method should be applied during the image generation. Supported modes: full, style, and composition

        ip_adapter_mode : typing.Optional[IpAdapterMode]
            Define which ip adapter mode should be applied during the image generation. Supported modes: base

        ip_adapter_scale : typing.Optional[float]
            How strong the effect of the ip adapter should be.

        loras : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            [Not supported on SD3] A dictionary of LoRAs to apply. LoRAs as keys and their weights (float) as values.

        mask_image : typing.Optional[str]
            [Not supported on SD3] b64 encoded mask image for inpainting. White area should indicate where to paint.

        negative_prompt : typing.Optional[str]
            Text describing image traits to avoid during generation.

        negative_prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high level description of things to avoid during generation. Used only by SD XL.

        num_images : typing.Optional[int]
            Integer representing how many output images to generate with a single prompt/configuration.

        outpainting : typing.Optional[bool]
            [Not supported on SD3] Whether the request requires outpainting or not. If so, special preprocessing is applied for better results.

        prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high-level description of the image to generate. Used only by SD XL.

        sampler : typing.Optional[Scheduler]
            [Not supported on SD3] Sampler name (also known as 'scheduler') to use during image generation.

        seed : typing.Optional[ImageGenerationRequestSeed]
            Integer number or list of integers representing the seeds of random generators. Fixing random seed is useful when attempting to generate a specific image. Must be greater than 0 and less than 2^32.

        steps : typing.Optional[int]
            Integer repreenting how many steps of diffusion to run. Must be greater than 0 and less than or equal to 200.

        strength : typing.Optional[float]
            Floating-point number indicating how much creative the Image to Image generation mode should be. Must be greater than 0 and less than or equal to 1.0.

        style_preset : typing.Optional[SdxlStyles]
            [Not supported on SD3] Pre-defined styles used to guide the output image towards a particular style. Pre-defined styles are only supported by SDXL.

        textual_inversions : typing.Optional[typing.Dict[str, typing.Optional[str]]]
            [Not supported on SD3] A dictionary of textual inversions to be used during image generation. Textual inversions as keys and trigger words as values.

        transfer_images : typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]]
            [Not supported on SD3] A dictionary containing a mapping of trigger words to a list of sample images which demonstrate the desired object or style to transfer.

        use_refiner : typing.Optional[bool]
            [Not supported on SD3] Whether to enable and apply the SDXL refiner model to the image generation.

        vae : typing.Optional[str]
            [Not Supported on SD3] Custom VAE to be used during image generation.

        width : typing.Optional[int]
            Integer representing the width of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ImageGenerationResponse
            Successful Response

        Examples
        --------
        from octoai.client import AsyncOctoAI

        client = AsyncOctoAI(
            api_key="YOUR_API_KEY",
        )
        await client.image_gen.generate_controlnet_sd15(
            prompt="prompt",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "generate/controlnet-sd15",
            base_url=self._client_wrapper.get_environment().image_gen,
            method="POST",
            json={
                "cfg_scale": cfg_scale,
                "checkpoint": checkpoint,
                "clip_skip": clip_skip,
                "controlnet": controlnet,
                "controlnet_conditioning_scale": controlnet_conditioning_scale,
                "controlnet_early_stop": controlnet_early_stop,
                "controlnet_image": controlnet_image,
                "controlnet_preprocess": controlnet_preprocess,
                "height": height,
                "high_noise_frac": high_noise_frac,
                "image_encoding": image_encoding,
                "init_image": init_image,
                "ip_adapter_image": ip_adapter_image,
                "ip_adapter_mask_image": ip_adapter_mask_image,
                "ip_adapter_method": ip_adapter_method,
                "ip_adapter_mode": ip_adapter_mode,
                "ip_adapter_scale": ip_adapter_scale,
                "loras": loras,
                "mask_image": mask_image,
                "negative_prompt": negative_prompt,
                "negative_prompt_2": negative_prompt2,
                "num_images": num_images,
                "outpainting": outpainting,
                "prompt": prompt,
                "prompt_2": prompt2,
                "sampler": sampler,
                "seed": seed,
                "steps": steps,
                "strength": strength,
                "style_preset": style_preset,
                "textual_inversions": textual_inversions,
                "transfer_images": transfer_images,
                "use_refiner": use_refiner,
                "vae": vae,
                "width": width,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(ImageGenerationResponse, _response.json())  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def generate_controlnet_sdxl(
        self,
        *,
        prompt: str,
        cfg_scale: typing.Optional[float] = OMIT,
        checkpoint: typing.Optional[str] = OMIT,
        clip_skip: typing.Optional[int] = OMIT,
        controlnet: typing.Optional[str] = OMIT,
        controlnet_conditioning_scale: typing.Optional[float] = OMIT,
        controlnet_early_stop: typing.Optional[float] = OMIT,
        controlnet_image: typing.Optional[str] = OMIT,
        controlnet_preprocess: typing.Optional[bool] = OMIT,
        height: typing.Optional[int] = OMIT,
        high_noise_frac: typing.Optional[float] = OMIT,
        image_encoding: typing.Optional[ImageEncoding] = OMIT,
        init_image: typing.Optional[str] = OMIT,
        ip_adapter_image: typing.Optional[str] = OMIT,
        ip_adapter_mask_image: typing.Optional[str] = OMIT,
        ip_adapter_method: typing.Optional[IpAdapterMethod] = OMIT,
        ip_adapter_mode: typing.Optional[IpAdapterMode] = OMIT,
        ip_adapter_scale: typing.Optional[float] = OMIT,
        loras: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        mask_image: typing.Optional[str] = OMIT,
        negative_prompt: typing.Optional[str] = OMIT,
        negative_prompt2: typing.Optional[str] = OMIT,
        num_images: typing.Optional[int] = OMIT,
        outpainting: typing.Optional[bool] = OMIT,
        prompt2: typing.Optional[str] = OMIT,
        sampler: typing.Optional[Scheduler] = OMIT,
        seed: typing.Optional[ImageGenerationRequestSeed] = OMIT,
        steps: typing.Optional[int] = OMIT,
        strength: typing.Optional[float] = OMIT,
        style_preset: typing.Optional[SdxlStyles] = OMIT,
        textual_inversions: typing.Optional[typing.Dict[str, typing.Optional[str]]] = OMIT,
        transfer_images: typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]] = OMIT,
        use_refiner: typing.Optional[bool] = OMIT,
        vae: typing.Optional[str] = OMIT,
        width: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> ImageGenerationResponse:
        """
        Generate images in response to the given request.

        Parameters
        ----------
        prompt : str
            Text describing the image content to generate.

        cfg_scale : typing.Optional[float]
            Floating-point number represeting how closely to adhere to prompt description. Must be a positive number no greater than 50.0.

        checkpoint : typing.Optional[str]
            [Not supported on SD3] Custom checkpoint to be used during image generation.

        clip_skip : typing.Optional[int]
            [Not supported on SD3] Optionally skip later layers of the text encoder. Higher values lead to more abstract interpretations of the prompt.

        controlnet : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] ControlNet to be used during image generation.

        controlnet_conditioning_scale : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] How strong the effect of the controlnet should be.

        controlnet_early_stop : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] If provided, indicates fraction of steps at which to stop applying controlnet. This can be used to sometimes generate better outputs.

        controlnet_image : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] Controlnet image encoded in b64 string for guiding image generation. Required for controlnet engines.

        controlnet_preprocess : typing.Optional[bool]
            [Not supported on SD3 or IPAdapter] Whether to apply automatic ControlNet preprocessing.

        height : typing.Optional[int]
            Integer representing the height of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        high_noise_frac : typing.Optional[float]
            [Not supported on SD3] Floating-point number that defines the fraction of steps to perform with the base model. Used only by SD XL. Must be greater than or equal to 0.0 and less than or equal to 1.0.

        image_encoding : typing.Optional[ImageEncoding]
            Define which encoding process should be applied before returning the generated image(s).

        init_image : typing.Optional[str]
            Starting point image encoded in b64 string for Image to Image generation mode.

        ip_adapter_image : typing.Optional[str]
            IP Adapter image encoded in b64 string for guiding image generation. Required for ip adapter engines.

        ip_adapter_mask_image : typing.Optional[str]
            b64 encoded mask image for ip adapter. White area should indicate where to paint.

        ip_adapter_method : typing.Optional[IpAdapterMethod]
            Define which ip adapter method should be applied during the image generation. Supported modes: full, style, and composition

        ip_adapter_mode : typing.Optional[IpAdapterMode]
            Define which ip adapter mode should be applied during the image generation. Supported modes: base

        ip_adapter_scale : typing.Optional[float]
            How strong the effect of the ip adapter should be.

        loras : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            [Not supported on SD3] A dictionary of LoRAs to apply. LoRAs as keys and their weights (float) as values.

        mask_image : typing.Optional[str]
            [Not supported on SD3] b64 encoded mask image for inpainting. White area should indicate where to paint.

        negative_prompt : typing.Optional[str]
            Text describing image traits to avoid during generation.

        negative_prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high level description of things to avoid during generation. Used only by SD XL.

        num_images : typing.Optional[int]
            Integer representing how many output images to generate with a single prompt/configuration.

        outpainting : typing.Optional[bool]
            [Not supported on SD3] Whether the request requires outpainting or not. If so, special preprocessing is applied for better results.

        prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high-level description of the image to generate. Used only by SD XL.

        sampler : typing.Optional[Scheduler]
            [Not supported on SD3] Sampler name (also known as 'scheduler') to use during image generation.

        seed : typing.Optional[ImageGenerationRequestSeed]
            Integer number or list of integers representing the seeds of random generators. Fixing random seed is useful when attempting to generate a specific image. Must be greater than 0 and less than 2^32.

        steps : typing.Optional[int]
            Integer repreenting how many steps of diffusion to run. Must be greater than 0 and less than or equal to 200.

        strength : typing.Optional[float]
            Floating-point number indicating how much creative the Image to Image generation mode should be. Must be greater than 0 and less than or equal to 1.0.

        style_preset : typing.Optional[SdxlStyles]
            [Not supported on SD3] Pre-defined styles used to guide the output image towards a particular style. Pre-defined styles are only supported by SDXL.

        textual_inversions : typing.Optional[typing.Dict[str, typing.Optional[str]]]
            [Not supported on SD3] A dictionary of textual inversions to be used during image generation. Textual inversions as keys and trigger words as values.

        transfer_images : typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]]
            [Not supported on SD3] A dictionary containing a mapping of trigger words to a list of sample images which demonstrate the desired object or style to transfer.

        use_refiner : typing.Optional[bool]
            [Not supported on SD3] Whether to enable and apply the SDXL refiner model to the image generation.

        vae : typing.Optional[str]
            [Not Supported on SD3] Custom VAE to be used during image generation.

        width : typing.Optional[int]
            Integer representing the width of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ImageGenerationResponse
            Successful Response

        Examples
        --------
        from octoai.client import AsyncOctoAI

        client = AsyncOctoAI(
            api_key="YOUR_API_KEY",
        )
        await client.image_gen.generate_controlnet_sdxl(
            prompt="prompt",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "generate/controlnet-sdxl",
            base_url=self._client_wrapper.get_environment().image_gen,
            method="POST",
            json={
                "cfg_scale": cfg_scale,
                "checkpoint": checkpoint,
                "clip_skip": clip_skip,
                "controlnet": controlnet,
                "controlnet_conditioning_scale": controlnet_conditioning_scale,
                "controlnet_early_stop": controlnet_early_stop,
                "controlnet_image": controlnet_image,
                "controlnet_preprocess": controlnet_preprocess,
                "height": height,
                "high_noise_frac": high_noise_frac,
                "image_encoding": image_encoding,
                "init_image": init_image,
                "ip_adapter_image": ip_adapter_image,
                "ip_adapter_mask_image": ip_adapter_mask_image,
                "ip_adapter_method": ip_adapter_method,
                "ip_adapter_mode": ip_adapter_mode,
                "ip_adapter_scale": ip_adapter_scale,
                "loras": loras,
                "mask_image": mask_image,
                "negative_prompt": negative_prompt,
                "negative_prompt_2": negative_prompt2,
                "num_images": num_images,
                "outpainting": outpainting,
                "prompt": prompt,
                "prompt_2": prompt2,
                "sampler": sampler,
                "seed": seed,
                "steps": steps,
                "strength": strength,
                "style_preset": style_preset,
                "textual_inversions": textual_inversions,
                "transfer_images": transfer_images,
                "use_refiner": use_refiner,
                "vae": vae,
                "width": width,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(ImageGenerationResponse, _response.json())  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def ip_adapter(
        self,
        *,
        prompt: str,
        cfg_scale: typing.Optional[float] = OMIT,
        checkpoint: typing.Optional[str] = OMIT,
        clip_skip: typing.Optional[int] = OMIT,
        controlnet: typing.Optional[str] = OMIT,
        controlnet_conditioning_scale: typing.Optional[float] = OMIT,
        controlnet_early_stop: typing.Optional[float] = OMIT,
        controlnet_image: typing.Optional[str] = OMIT,
        controlnet_preprocess: typing.Optional[bool] = OMIT,
        height: typing.Optional[int] = OMIT,
        high_noise_frac: typing.Optional[float] = OMIT,
        image_encoding: typing.Optional[ImageEncoding] = OMIT,
        init_image: typing.Optional[str] = OMIT,
        ip_adapter_image: typing.Optional[str] = OMIT,
        ip_adapter_mask_image: typing.Optional[str] = OMIT,
        ip_adapter_method: typing.Optional[IpAdapterMethod] = OMIT,
        ip_adapter_mode: typing.Optional[IpAdapterMode] = OMIT,
        ip_adapter_scale: typing.Optional[float] = OMIT,
        loras: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        mask_image: typing.Optional[str] = OMIT,
        negative_prompt: typing.Optional[str] = OMIT,
        negative_prompt2: typing.Optional[str] = OMIT,
        num_images: typing.Optional[int] = OMIT,
        outpainting: typing.Optional[bool] = OMIT,
        prompt2: typing.Optional[str] = OMIT,
        sampler: typing.Optional[Scheduler] = OMIT,
        seed: typing.Optional[ImageGenerationRequestSeed] = OMIT,
        steps: typing.Optional[int] = OMIT,
        strength: typing.Optional[float] = OMIT,
        style_preset: typing.Optional[SdxlStyles] = OMIT,
        textual_inversions: typing.Optional[typing.Dict[str, typing.Optional[str]]] = OMIT,
        transfer_images: typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]] = OMIT,
        use_refiner: typing.Optional[bool] = OMIT,
        vae: typing.Optional[str] = OMIT,
        width: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> ImageGenerationResponse:
        """
        Generate images in response to the given request.

        Parameters
        ----------
        prompt : str
            Text describing the image content to generate.

        cfg_scale : typing.Optional[float]
            Floating-point number represeting how closely to adhere to prompt description. Must be a positive number no greater than 50.0.

        checkpoint : typing.Optional[str]
            [Not supported on SD3] Custom checkpoint to be used during image generation.

        clip_skip : typing.Optional[int]
            [Not supported on SD3] Optionally skip later layers of the text encoder. Higher values lead to more abstract interpretations of the prompt.

        controlnet : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] ControlNet to be used during image generation.

        controlnet_conditioning_scale : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] How strong the effect of the controlnet should be.

        controlnet_early_stop : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] If provided, indicates fraction of steps at which to stop applying controlnet. This can be used to sometimes generate better outputs.

        controlnet_image : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] Controlnet image encoded in b64 string for guiding image generation. Required for controlnet engines.

        controlnet_preprocess : typing.Optional[bool]
            [Not supported on SD3 or IPAdapter] Whether to apply automatic ControlNet preprocessing.

        height : typing.Optional[int]
            Integer representing the height of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        high_noise_frac : typing.Optional[float]
            [Not supported on SD3] Floating-point number that defines the fraction of steps to perform with the base model. Used only by SD XL. Must be greater than or equal to 0.0 and less than or equal to 1.0.

        image_encoding : typing.Optional[ImageEncoding]
            Define which encoding process should be applied before returning the generated image(s).

        init_image : typing.Optional[str]
            Starting point image encoded in b64 string for Image to Image generation mode.

        ip_adapter_image : typing.Optional[str]
            IP Adapter image encoded in b64 string for guiding image generation. Required for ip adapter engines.

        ip_adapter_mask_image : typing.Optional[str]
            b64 encoded mask image for ip adapter. White area should indicate where to paint.

        ip_adapter_method : typing.Optional[IpAdapterMethod]
            Define which ip adapter method should be applied during the image generation. Supported modes: full, style, and composition

        ip_adapter_mode : typing.Optional[IpAdapterMode]
            Define which ip adapter mode should be applied during the image generation. Supported modes: base

        ip_adapter_scale : typing.Optional[float]
            How strong the effect of the ip adapter should be.

        loras : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            [Not supported on SD3] A dictionary of LoRAs to apply. LoRAs as keys and their weights (float) as values.

        mask_image : typing.Optional[str]
            [Not supported on SD3] b64 encoded mask image for inpainting. White area should indicate where to paint.

        negative_prompt : typing.Optional[str]
            Text describing image traits to avoid during generation.

        negative_prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high level description of things to avoid during generation. Used only by SD XL.

        num_images : typing.Optional[int]
            Integer representing how many output images to generate with a single prompt/configuration.

        outpainting : typing.Optional[bool]
            [Not supported on SD3] Whether the request requires outpainting or not. If so, special preprocessing is applied for better results.

        prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high-level description of the image to generate. Used only by SD XL.

        sampler : typing.Optional[Scheduler]
            [Not supported on SD3] Sampler name (also known as 'scheduler') to use during image generation.

        seed : typing.Optional[ImageGenerationRequestSeed]
            Integer number or list of integers representing the seeds of random generators. Fixing random seed is useful when attempting to generate a specific image. Must be greater than 0 and less than 2^32.

        steps : typing.Optional[int]
            Integer repreenting how many steps of diffusion to run. Must be greater than 0 and less than or equal to 200.

        strength : typing.Optional[float]
            Floating-point number indicating how much creative the Image to Image generation mode should be. Must be greater than 0 and less than or equal to 1.0.

        style_preset : typing.Optional[SdxlStyles]
            [Not supported on SD3] Pre-defined styles used to guide the output image towards a particular style. Pre-defined styles are only supported by SDXL.

        textual_inversions : typing.Optional[typing.Dict[str, typing.Optional[str]]]
            [Not supported on SD3] A dictionary of textual inversions to be used during image generation. Textual inversions as keys and trigger words as values.

        transfer_images : typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]]
            [Not supported on SD3] A dictionary containing a mapping of trigger words to a list of sample images which demonstrate the desired object or style to transfer.

        use_refiner : typing.Optional[bool]
            [Not supported on SD3] Whether to enable and apply the SDXL refiner model to the image generation.

        vae : typing.Optional[str]
            [Not Supported on SD3] Custom VAE to be used during image generation.

        width : typing.Optional[int]
            Integer representing the width of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ImageGenerationResponse
            Successful Response

        Examples
        --------
        from octoai.client import AsyncOctoAI

        client = AsyncOctoAI(
            api_key="YOUR_API_KEY",
        )
        await client.image_gen.ip_adapter(
            prompt="prompt",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "generate/ip-adapter-sdxl",
            base_url=self._client_wrapper.get_environment().image_gen,
            method="POST",
            json={
                "cfg_scale": cfg_scale,
                "checkpoint": checkpoint,
                "clip_skip": clip_skip,
                "controlnet": controlnet,
                "controlnet_conditioning_scale": controlnet_conditioning_scale,
                "controlnet_early_stop": controlnet_early_stop,
                "controlnet_image": controlnet_image,
                "controlnet_preprocess": controlnet_preprocess,
                "height": height,
                "high_noise_frac": high_noise_frac,
                "image_encoding": image_encoding,
                "init_image": init_image,
                "ip_adapter_image": ip_adapter_image,
                "ip_adapter_mask_image": ip_adapter_mask_image,
                "ip_adapter_method": ip_adapter_method,
                "ip_adapter_mode": ip_adapter_mode,
                "ip_adapter_scale": ip_adapter_scale,
                "loras": loras,
                "mask_image": mask_image,
                "negative_prompt": negative_prompt,
                "negative_prompt_2": negative_prompt2,
                "num_images": num_images,
                "outpainting": outpainting,
                "prompt": prompt,
                "prompt_2": prompt2,
                "sampler": sampler,
                "seed": seed,
                "steps": steps,
                "strength": strength,
                "style_preset": style_preset,
                "textual_inversions": textual_inversions,
                "transfer_images": transfer_images,
                "use_refiner": use_refiner,
                "vae": vae,
                "width": width,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(ImageGenerationResponse, _response.json())  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def generate_sd(
        self,
        *,
        prompt: str,
        cfg_scale: typing.Optional[float] = OMIT,
        checkpoint: typing.Optional[str] = OMIT,
        clip_skip: typing.Optional[int] = OMIT,
        controlnet: typing.Optional[str] = OMIT,
        controlnet_conditioning_scale: typing.Optional[float] = OMIT,
        controlnet_early_stop: typing.Optional[float] = OMIT,
        controlnet_image: typing.Optional[str] = OMIT,
        controlnet_preprocess: typing.Optional[bool] = OMIT,
        height: typing.Optional[int] = OMIT,
        high_noise_frac: typing.Optional[float] = OMIT,
        image_encoding: typing.Optional[ImageEncoding] = OMIT,
        init_image: typing.Optional[str] = OMIT,
        ip_adapter_image: typing.Optional[str] = OMIT,
        ip_adapter_mask_image: typing.Optional[str] = OMIT,
        ip_adapter_method: typing.Optional[IpAdapterMethod] = OMIT,
        ip_adapter_mode: typing.Optional[IpAdapterMode] = OMIT,
        ip_adapter_scale: typing.Optional[float] = OMIT,
        loras: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        mask_image: typing.Optional[str] = OMIT,
        negative_prompt: typing.Optional[str] = OMIT,
        negative_prompt2: typing.Optional[str] = OMIT,
        num_images: typing.Optional[int] = OMIT,
        outpainting: typing.Optional[bool] = OMIT,
        prompt2: typing.Optional[str] = OMIT,
        sampler: typing.Optional[Scheduler] = OMIT,
        seed: typing.Optional[ImageGenerationRequestSeed] = OMIT,
        steps: typing.Optional[int] = OMIT,
        strength: typing.Optional[float] = OMIT,
        style_preset: typing.Optional[SdxlStyles] = OMIT,
        textual_inversions: typing.Optional[typing.Dict[str, typing.Optional[str]]] = OMIT,
        transfer_images: typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]] = OMIT,
        use_refiner: typing.Optional[bool] = OMIT,
        vae: typing.Optional[str] = OMIT,
        width: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> ImageGenerationResponse:
        """
        Generate images in response to the given request.

        Parameters
        ----------
        prompt : str
            Text describing the image content to generate.

        cfg_scale : typing.Optional[float]
            Floating-point number represeting how closely to adhere to prompt description. Must be a positive number no greater than 50.0.

        checkpoint : typing.Optional[str]
            [Not supported on SD3] Custom checkpoint to be used during image generation.

        clip_skip : typing.Optional[int]
            [Not supported on SD3] Optionally skip later layers of the text encoder. Higher values lead to more abstract interpretations of the prompt.

        controlnet : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] ControlNet to be used during image generation.

        controlnet_conditioning_scale : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] How strong the effect of the controlnet should be.

        controlnet_early_stop : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] If provided, indicates fraction of steps at which to stop applying controlnet. This can be used to sometimes generate better outputs.

        controlnet_image : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] Controlnet image encoded in b64 string for guiding image generation. Required for controlnet engines.

        controlnet_preprocess : typing.Optional[bool]
            [Not supported on SD3 or IPAdapter] Whether to apply automatic ControlNet preprocessing.

        height : typing.Optional[int]
            Integer representing the height of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        high_noise_frac : typing.Optional[float]
            [Not supported on SD3] Floating-point number that defines the fraction of steps to perform with the base model. Used only by SD XL. Must be greater than or equal to 0.0 and less than or equal to 1.0.

        image_encoding : typing.Optional[ImageEncoding]
            Define which encoding process should be applied before returning the generated image(s).

        init_image : typing.Optional[str]
            Starting point image encoded in b64 string for Image to Image generation mode.

        ip_adapter_image : typing.Optional[str]
            IP Adapter image encoded in b64 string for guiding image generation. Required for ip adapter engines.

        ip_adapter_mask_image : typing.Optional[str]
            b64 encoded mask image for ip adapter. White area should indicate where to paint.

        ip_adapter_method : typing.Optional[IpAdapterMethod]
            Define which ip adapter method should be applied during the image generation. Supported modes: full, style, and composition

        ip_adapter_mode : typing.Optional[IpAdapterMode]
            Define which ip adapter mode should be applied during the image generation. Supported modes: base

        ip_adapter_scale : typing.Optional[float]
            How strong the effect of the ip adapter should be.

        loras : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            [Not supported on SD3] A dictionary of LoRAs to apply. LoRAs as keys and their weights (float) as values.

        mask_image : typing.Optional[str]
            [Not supported on SD3] b64 encoded mask image for inpainting. White area should indicate where to paint.

        negative_prompt : typing.Optional[str]
            Text describing image traits to avoid during generation.

        negative_prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high level description of things to avoid during generation. Used only by SD XL.

        num_images : typing.Optional[int]
            Integer representing how many output images to generate with a single prompt/configuration.

        outpainting : typing.Optional[bool]
            [Not supported on SD3] Whether the request requires outpainting or not. If so, special preprocessing is applied for better results.

        prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high-level description of the image to generate. Used only by SD XL.

        sampler : typing.Optional[Scheduler]
            [Not supported on SD3] Sampler name (also known as 'scheduler') to use during image generation.

        seed : typing.Optional[ImageGenerationRequestSeed]
            Integer number or list of integers representing the seeds of random generators. Fixing random seed is useful when attempting to generate a specific image. Must be greater than 0 and less than 2^32.

        steps : typing.Optional[int]
            Integer repreenting how many steps of diffusion to run. Must be greater than 0 and less than or equal to 200.

        strength : typing.Optional[float]
            Floating-point number indicating how much creative the Image to Image generation mode should be. Must be greater than 0 and less than or equal to 1.0.

        style_preset : typing.Optional[SdxlStyles]
            [Not supported on SD3] Pre-defined styles used to guide the output image towards a particular style. Pre-defined styles are only supported by SDXL.

        textual_inversions : typing.Optional[typing.Dict[str, typing.Optional[str]]]
            [Not supported on SD3] A dictionary of textual inversions to be used during image generation. Textual inversions as keys and trigger words as values.

        transfer_images : typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]]
            [Not supported on SD3] A dictionary containing a mapping of trigger words to a list of sample images which demonstrate the desired object or style to transfer.

        use_refiner : typing.Optional[bool]
            [Not supported on SD3] Whether to enable and apply the SDXL refiner model to the image generation.

        vae : typing.Optional[str]
            [Not Supported on SD3] Custom VAE to be used during image generation.

        width : typing.Optional[int]
            Integer representing the width of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ImageGenerationResponse
            Successful Response

        Examples
        --------
        from octoai.client import AsyncOctoAI

        client = AsyncOctoAI(
            api_key="YOUR_API_KEY",
        )
        await client.image_gen.generate_sd(
            prompt="prompt",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "generate/sd",
            base_url=self._client_wrapper.get_environment().image_gen,
            method="POST",
            json={
                "cfg_scale": cfg_scale,
                "checkpoint": checkpoint,
                "clip_skip": clip_skip,
                "controlnet": controlnet,
                "controlnet_conditioning_scale": controlnet_conditioning_scale,
                "controlnet_early_stop": controlnet_early_stop,
                "controlnet_image": controlnet_image,
                "controlnet_preprocess": controlnet_preprocess,
                "height": height,
                "high_noise_frac": high_noise_frac,
                "image_encoding": image_encoding,
                "init_image": init_image,
                "ip_adapter_image": ip_adapter_image,
                "ip_adapter_mask_image": ip_adapter_mask_image,
                "ip_adapter_method": ip_adapter_method,
                "ip_adapter_mode": ip_adapter_mode,
                "ip_adapter_scale": ip_adapter_scale,
                "loras": loras,
                "mask_image": mask_image,
                "negative_prompt": negative_prompt,
                "negative_prompt_2": negative_prompt2,
                "num_images": num_images,
                "outpainting": outpainting,
                "prompt": prompt,
                "prompt_2": prompt2,
                "sampler": sampler,
                "seed": seed,
                "steps": steps,
                "strength": strength,
                "style_preset": style_preset,
                "textual_inversions": textual_inversions,
                "transfer_images": transfer_images,
                "use_refiner": use_refiner,
                "vae": vae,
                "width": width,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(ImageGenerationResponse, _response.json())  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def generate_sd3(
        self,
        *,
        prompt: str,
        cfg_scale: typing.Optional[float] = OMIT,
        checkpoint: typing.Optional[str] = OMIT,
        clip_skip: typing.Optional[int] = OMIT,
        controlnet: typing.Optional[str] = OMIT,
        controlnet_conditioning_scale: typing.Optional[float] = OMIT,
        controlnet_early_stop: typing.Optional[float] = OMIT,
        controlnet_image: typing.Optional[str] = OMIT,
        controlnet_preprocess: typing.Optional[bool] = OMIT,
        height: typing.Optional[int] = OMIT,
        high_noise_frac: typing.Optional[float] = OMIT,
        image_encoding: typing.Optional[ImageEncoding] = OMIT,
        init_image: typing.Optional[str] = OMIT,
        ip_adapter_image: typing.Optional[str] = OMIT,
        ip_adapter_mask_image: typing.Optional[str] = OMIT,
        ip_adapter_method: typing.Optional[IpAdapterMethod] = OMIT,
        ip_adapter_mode: typing.Optional[IpAdapterMode] = OMIT,
        ip_adapter_scale: typing.Optional[float] = OMIT,
        loras: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        mask_image: typing.Optional[str] = OMIT,
        negative_prompt: typing.Optional[str] = OMIT,
        negative_prompt2: typing.Optional[str] = OMIT,
        num_images: typing.Optional[int] = OMIT,
        outpainting: typing.Optional[bool] = OMIT,
        prompt2: typing.Optional[str] = OMIT,
        sampler: typing.Optional[Scheduler] = OMIT,
        seed: typing.Optional[ImageGenerationRequestSeed] = OMIT,
        steps: typing.Optional[int] = OMIT,
        strength: typing.Optional[float] = OMIT,
        style_preset: typing.Optional[SdxlStyles] = OMIT,
        textual_inversions: typing.Optional[typing.Dict[str, typing.Optional[str]]] = OMIT,
        transfer_images: typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]] = OMIT,
        use_refiner: typing.Optional[bool] = OMIT,
        vae: typing.Optional[str] = OMIT,
        width: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> ImageGenerationResponse:
        """
        Generate images in response to the given request.

        Parameters
        ----------
        prompt : str
            Text describing the image content to generate.

        cfg_scale : typing.Optional[float]
            Floating-point number represeting how closely to adhere to prompt description. Must be a positive number no greater than 50.0.

        checkpoint : typing.Optional[str]
            [Not supported on SD3] Custom checkpoint to be used during image generation.

        clip_skip : typing.Optional[int]
            [Not supported on SD3] Optionally skip later layers of the text encoder. Higher values lead to more abstract interpretations of the prompt.

        controlnet : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] ControlNet to be used during image generation.

        controlnet_conditioning_scale : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] How strong the effect of the controlnet should be.

        controlnet_early_stop : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] If provided, indicates fraction of steps at which to stop applying controlnet. This can be used to sometimes generate better outputs.

        controlnet_image : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] Controlnet image encoded in b64 string for guiding image generation. Required for controlnet engines.

        controlnet_preprocess : typing.Optional[bool]
            [Not supported on SD3 or IPAdapter] Whether to apply automatic ControlNet preprocessing.

        height : typing.Optional[int]
            Integer representing the height of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        high_noise_frac : typing.Optional[float]
            [Not supported on SD3] Floating-point number that defines the fraction of steps to perform with the base model. Used only by SD XL. Must be greater than or equal to 0.0 and less than or equal to 1.0.

        image_encoding : typing.Optional[ImageEncoding]
            Define which encoding process should be applied before returning the generated image(s).

        init_image : typing.Optional[str]
            Starting point image encoded in b64 string for Image to Image generation mode.

        ip_adapter_image : typing.Optional[str]
            IP Adapter image encoded in b64 string for guiding image generation. Required for ip adapter engines.

        ip_adapter_mask_image : typing.Optional[str]
            b64 encoded mask image for ip adapter. White area should indicate where to paint.

        ip_adapter_method : typing.Optional[IpAdapterMethod]
            Define which ip adapter method should be applied during the image generation. Supported modes: full, style, and composition

        ip_adapter_mode : typing.Optional[IpAdapterMode]
            Define which ip adapter mode should be applied during the image generation. Supported modes: base

        ip_adapter_scale : typing.Optional[float]
            How strong the effect of the ip adapter should be.

        loras : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            [Not supported on SD3] A dictionary of LoRAs to apply. LoRAs as keys and their weights (float) as values.

        mask_image : typing.Optional[str]
            [Not supported on SD3] b64 encoded mask image for inpainting. White area should indicate where to paint.

        negative_prompt : typing.Optional[str]
            Text describing image traits to avoid during generation.

        negative_prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high level description of things to avoid during generation. Used only by SD XL.

        num_images : typing.Optional[int]
            Integer representing how many output images to generate with a single prompt/configuration.

        outpainting : typing.Optional[bool]
            [Not supported on SD3] Whether the request requires outpainting or not. If so, special preprocessing is applied for better results.

        prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high-level description of the image to generate. Used only by SD XL.

        sampler : typing.Optional[Scheduler]
            [Not supported on SD3] Sampler name (also known as 'scheduler') to use during image generation.

        seed : typing.Optional[ImageGenerationRequestSeed]
            Integer number or list of integers representing the seeds of random generators. Fixing random seed is useful when attempting to generate a specific image. Must be greater than 0 and less than 2^32.

        steps : typing.Optional[int]
            Integer repreenting how many steps of diffusion to run. Must be greater than 0 and less than or equal to 200.

        strength : typing.Optional[float]
            Floating-point number indicating how much creative the Image to Image generation mode should be. Must be greater than 0 and less than or equal to 1.0.

        style_preset : typing.Optional[SdxlStyles]
            [Not supported on SD3] Pre-defined styles used to guide the output image towards a particular style. Pre-defined styles are only supported by SDXL.

        textual_inversions : typing.Optional[typing.Dict[str, typing.Optional[str]]]
            [Not supported on SD3] A dictionary of textual inversions to be used during image generation. Textual inversions as keys and trigger words as values.

        transfer_images : typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]]
            [Not supported on SD3] A dictionary containing a mapping of trigger words to a list of sample images which demonstrate the desired object or style to transfer.

        use_refiner : typing.Optional[bool]
            [Not supported on SD3] Whether to enable and apply the SDXL refiner model to the image generation.

        vae : typing.Optional[str]
            [Not Supported on SD3] Custom VAE to be used during image generation.

        width : typing.Optional[int]
            Integer representing the width of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ImageGenerationResponse
            Successful Response

        Examples
        --------
        from octoai.client import AsyncOctoAI

        client = AsyncOctoAI(
            api_key="YOUR_API_KEY",
        )
        await client.image_gen.generate_sd3(
            prompt="prompt",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "generate/sd3",
            base_url=self._client_wrapper.get_environment().image_gen,
            method="POST",
            json={
                "cfg_scale": cfg_scale,
                "checkpoint": checkpoint,
                "clip_skip": clip_skip,
                "controlnet": controlnet,
                "controlnet_conditioning_scale": controlnet_conditioning_scale,
                "controlnet_early_stop": controlnet_early_stop,
                "controlnet_image": controlnet_image,
                "controlnet_preprocess": controlnet_preprocess,
                "height": height,
                "high_noise_frac": high_noise_frac,
                "image_encoding": image_encoding,
                "init_image": init_image,
                "ip_adapter_image": ip_adapter_image,
                "ip_adapter_mask_image": ip_adapter_mask_image,
                "ip_adapter_method": ip_adapter_method,
                "ip_adapter_mode": ip_adapter_mode,
                "ip_adapter_scale": ip_adapter_scale,
                "loras": loras,
                "mask_image": mask_image,
                "negative_prompt": negative_prompt,
                "negative_prompt_2": negative_prompt2,
                "num_images": num_images,
                "outpainting": outpainting,
                "prompt": prompt,
                "prompt_2": prompt2,
                "sampler": sampler,
                "seed": seed,
                "steps": steps,
                "strength": strength,
                "style_preset": style_preset,
                "textual_inversions": textual_inversions,
                "transfer_images": transfer_images,
                "use_refiner": use_refiner,
                "vae": vae,
                "width": width,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(ImageGenerationResponse, _response.json())  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def generate_sdxl(
        self,
        *,
        prompt: str,
        cfg_scale: typing.Optional[float] = OMIT,
        checkpoint: typing.Optional[str] = OMIT,
        clip_skip: typing.Optional[int] = OMIT,
        controlnet: typing.Optional[str] = OMIT,
        controlnet_conditioning_scale: typing.Optional[float] = OMIT,
        controlnet_early_stop: typing.Optional[float] = OMIT,
        controlnet_image: typing.Optional[str] = OMIT,
        controlnet_preprocess: typing.Optional[bool] = OMIT,
        height: typing.Optional[int] = OMIT,
        high_noise_frac: typing.Optional[float] = OMIT,
        image_encoding: typing.Optional[ImageEncoding] = OMIT,
        init_image: typing.Optional[str] = OMIT,
        ip_adapter_image: typing.Optional[str] = OMIT,
        ip_adapter_mask_image: typing.Optional[str] = OMIT,
        ip_adapter_method: typing.Optional[IpAdapterMethod] = OMIT,
        ip_adapter_mode: typing.Optional[IpAdapterMode] = OMIT,
        ip_adapter_scale: typing.Optional[float] = OMIT,
        loras: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        mask_image: typing.Optional[str] = OMIT,
        negative_prompt: typing.Optional[str] = OMIT,
        negative_prompt2: typing.Optional[str] = OMIT,
        num_images: typing.Optional[int] = OMIT,
        outpainting: typing.Optional[bool] = OMIT,
        prompt2: typing.Optional[str] = OMIT,
        sampler: typing.Optional[Scheduler] = OMIT,
        seed: typing.Optional[ImageGenerationRequestSeed] = OMIT,
        steps: typing.Optional[int] = OMIT,
        strength: typing.Optional[float] = OMIT,
        style_preset: typing.Optional[SdxlStyles] = OMIT,
        textual_inversions: typing.Optional[typing.Dict[str, typing.Optional[str]]] = OMIT,
        transfer_images: typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]] = OMIT,
        use_refiner: typing.Optional[bool] = OMIT,
        vae: typing.Optional[str] = OMIT,
        width: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> ImageGenerationResponse:
        """
        Generate images in response to the given request.

        Parameters
        ----------
        prompt : str
            Text describing the image content to generate.

        cfg_scale : typing.Optional[float]
            Floating-point number represeting how closely to adhere to prompt description. Must be a positive number no greater than 50.0.

        checkpoint : typing.Optional[str]
            [Not supported on SD3] Custom checkpoint to be used during image generation.

        clip_skip : typing.Optional[int]
            [Not supported on SD3] Optionally skip later layers of the text encoder. Higher values lead to more abstract interpretations of the prompt.

        controlnet : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] ControlNet to be used during image generation.

        controlnet_conditioning_scale : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] How strong the effect of the controlnet should be.

        controlnet_early_stop : typing.Optional[float]
            [Not supported on SD3 or IPAdapter] If provided, indicates fraction of steps at which to stop applying controlnet. This can be used to sometimes generate better outputs.

        controlnet_image : typing.Optional[str]
            [Not supported on SD3 or IPAdapter] Controlnet image encoded in b64 string for guiding image generation. Required for controlnet engines.

        controlnet_preprocess : typing.Optional[bool]
            [Not supported on SD3 or IPAdapter] Whether to apply automatic ControlNet preprocessing.

        height : typing.Optional[int]
            Integer representing the height of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        high_noise_frac : typing.Optional[float]
            [Not supported on SD3] Floating-point number that defines the fraction of steps to perform with the base model. Used only by SD XL. Must be greater than or equal to 0.0 and less than or equal to 1.0.

        image_encoding : typing.Optional[ImageEncoding]
            Define which encoding process should be applied before returning the generated image(s).

        init_image : typing.Optional[str]
            Starting point image encoded in b64 string for Image to Image generation mode.

        ip_adapter_image : typing.Optional[str]
            IP Adapter image encoded in b64 string for guiding image generation. Required for ip adapter engines.

        ip_adapter_mask_image : typing.Optional[str]
            b64 encoded mask image for ip adapter. White area should indicate where to paint.

        ip_adapter_method : typing.Optional[IpAdapterMethod]
            Define which ip adapter method should be applied during the image generation. Supported modes: full, style, and composition

        ip_adapter_mode : typing.Optional[IpAdapterMode]
            Define which ip adapter mode should be applied during the image generation. Supported modes: base

        ip_adapter_scale : typing.Optional[float]
            How strong the effect of the ip adapter should be.

        loras : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            [Not supported on SD3] A dictionary of LoRAs to apply. LoRAs as keys and their weights (float) as values.

        mask_image : typing.Optional[str]
            [Not supported on SD3] b64 encoded mask image for inpainting. White area should indicate where to paint.

        negative_prompt : typing.Optional[str]
            Text describing image traits to avoid during generation.

        negative_prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high level description of things to avoid during generation. Used only by SD XL.

        num_images : typing.Optional[int]
            Integer representing how many output images to generate with a single prompt/configuration.

        outpainting : typing.Optional[bool]
            [Not supported on SD3] Whether the request requires outpainting or not. If so, special preprocessing is applied for better results.

        prompt2 : typing.Optional[str]
            [Not supported on SD3] Text with a high-level description of the image to generate. Used only by SD XL.

        sampler : typing.Optional[Scheduler]
            [Not supported on SD3] Sampler name (also known as 'scheduler') to use during image generation.

        seed : typing.Optional[ImageGenerationRequestSeed]
            Integer number or list of integers representing the seeds of random generators. Fixing random seed is useful when attempting to generate a specific image. Must be greater than 0 and less than 2^32.

        steps : typing.Optional[int]
            Integer repreenting how many steps of diffusion to run. Must be greater than 0 and less than or equal to 200.

        strength : typing.Optional[float]
            Floating-point number indicating how much creative the Image to Image generation mode should be. Must be greater than 0 and less than or equal to 1.0.

        style_preset : typing.Optional[SdxlStyles]
            [Not supported on SD3] Pre-defined styles used to guide the output image towards a particular style. Pre-defined styles are only supported by SDXL.

        textual_inversions : typing.Optional[typing.Dict[str, typing.Optional[str]]]
            [Not supported on SD3] A dictionary of textual inversions to be used during image generation. Textual inversions as keys and trigger words as values.

        transfer_images : typing.Optional[typing.Dict[str, typing.Optional[typing.Sequence[str]]]]
            [Not supported on SD3] A dictionary containing a mapping of trigger words to a list of sample images which demonstrate the desired object or style to transfer.

        use_refiner : typing.Optional[bool]
            [Not supported on SD3] Whether to enable and apply the SDXL refiner model to the image generation.

        vae : typing.Optional[str]
            [Not Supported on SD3] Custom VAE to be used during image generation.

        width : typing.Optional[int]
            Integer representing the width of image to generate. None will default to 512 for SD 1.5 and 1024 for SD3, SDXL, and SSD. Supported resolutions (w,h): SD3={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SDXL={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}, SD1.5={(768, 576), (1024, 576), (640, 512), (384, 704), (640, 768), (640, 640), (1024, 768), (1536, 1024), (768, 1024), (576, 448), (1024, 1024), (896, 896), (704, 1216), (512, 512), (448, 576), (832, 512), (512, 704), (576, 768), (1216, 704), (512, 768), (512, 832), (1024, 1536), (576, 1024), (704, 384), (768, 512)}, SSD={(1536, 640), (768, 1344), (832, 1216), (1344, 768), (1152, 896), (640, 1536), (1216, 832), (896, 1152), (1024, 1024)}.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ImageGenerationResponse
            Successful Response

        Examples
        --------
        from octoai.client import AsyncOctoAI

        client = AsyncOctoAI(
            api_key="YOUR_API_KEY",
        )
        await client.image_gen.generate_sdxl(
            prompt="prompt",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "generate/sdxl",
            base_url=self._client_wrapper.get_environment().image_gen,
            method="POST",
            json={
                "cfg_scale": cfg_scale,
                "checkpoint": checkpoint,
                "clip_skip": clip_skip,
                "controlnet": controlnet,
                "controlnet_conditioning_scale": controlnet_conditioning_scale,
                "controlnet_early_stop": controlnet_early_stop,
                "controlnet_image": controlnet_image,
                "controlnet_preprocess": controlnet_preprocess,
                "height": height,
                "high_noise_frac": high_noise_frac,
                "image_encoding": image_encoding,
                "init_image": init_image,
                "ip_adapter_image": ip_adapter_image,
                "ip_adapter_mask_image": ip_adapter_mask_image,
                "ip_adapter_method": ip_adapter_method,
                "ip_adapter_mode": ip_adapter_mode,
                "ip_adapter_scale": ip_adapter_scale,
                "loras": loras,
                "mask_image": mask_image,
                "negative_prompt": negative_prompt,
                "negative_prompt_2": negative_prompt2,
                "num_images": num_images,
                "outpainting": outpainting,
                "prompt": prompt,
                "prompt_2": prompt2,
                "sampler": sampler,
                "seed": seed,
                "steps": steps,
                "strength": strength,
                "style_preset": style_preset,
                "textual_inversions": textual_inversions,
                "transfer_images": transfer_images,
                "use_refiner": use_refiner,
                "vae": vae,
                "width": width,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(ImageGenerationResponse, _response.json())  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def generate_svd(
        self,
        *,
        image: str,
        cfg_scale: typing.Optional[float] = OMIT,
        constant_rate_factor: typing.Optional[int] = OMIT,
        fps: typing.Optional[int] = OMIT,
        height: typing.Optional[int] = OMIT,
        motion_scale: typing.Optional[float] = OMIT,
        noise_aug_strength: typing.Optional[float] = OMIT,
        num_videos: typing.Optional[int] = OMIT,
        seed: typing.Optional[VideoGenerationRequestSeed] = OMIT,
        steps: typing.Optional[int] = OMIT,
        width: typing.Optional[int] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> VideoGenerationResponse:
        """
        Generate videos in response to the given request.

        Parameters
        ----------
        image : str
            Starting point image encoded in base64 string.

        cfg_scale : typing.Optional[float]
            Floating-point number represeting how closely to adhere to 'image'. Must be a positive number no greater than 10.0.

        constant_rate_factor : typing.Optional[int]
            Integer representing the quality of the video encoding. Higher quality means the file will be larger.The range of the CRF scale is [0,51], where 0 is lossless, 23 is the default, and 51 is worst quality possible.

        fps : typing.Optional[int]
            Integer representing how fast the generated frames should play back.

        height : typing.Optional[int]
            Integer representing the height of video/animation to generate.If not provided, the output height will be inferred from the input 'image', and the closest resolution supported will be chosen. Supported resolutions (w,h): {(576, 1024), (1024, 576), (768, 768)}.

        motion_scale : typing.Optional[float]
            A floating point number between 0.0 and 1.0 indicating how much motion should be in the generated video/animation.

        noise_aug_strength : typing.Optional[float]
            A floating point number between 0.0 and 1.0 indicatiing how much noise to add to the initial image. Higher values encourage creativity.

        num_videos : typing.Optional[int]
            Integer representing how many output videos/animations to generate with a single 'image' and configuration.

        seed : typing.Optional[VideoGenerationRequestSeed]
            Integer number or list of integers representing the seeds of random generators.Fixing random seed is useful when attempting to generate a specific video/animation (or set of videos/animations). Must be greater than 0 and less than 2^32.

        steps : typing.Optional[int]
            Integer repreenting how many steps of diffusion to run. Must be greater than 0 and less than or equal to 50.

        width : typing.Optional[int]
            Integer representing the width of video/animation to generate.If not provided, the output width will be inferred from the input 'image', and the closest resolution supported will be chosen. Supported resolutions (w,h): {(576, 1024), (1024, 576), (768, 768)}.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        VideoGenerationResponse
            Successful Response

        Examples
        --------
        from octoai.client import AsyncOctoAI

        client = AsyncOctoAI(
            api_key="YOUR_API_KEY",
        )
        await client.image_gen.generate_svd(
            image="image",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "generate/svd",
            base_url=self._client_wrapper.get_environment().image_gen,
            method="POST",
            json={
                "cfg_scale": cfg_scale,
                "constant_rate_factor": constant_rate_factor,
                "fps": fps,
                "height": height,
                "image": image,
                "motion_scale": motion_scale,
                "noise_aug_strength": noise_aug_strength,
                "num_videos": num_videos,
                "seed": seed,
                "steps": steps,
                "width": width,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(VideoGenerationResponse, _response.json())  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
