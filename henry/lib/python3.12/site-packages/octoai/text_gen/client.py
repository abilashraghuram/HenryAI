# This file was auto-generated by Fern from our API Definition.

import json
import typing
from json.decoder import JSONDecodeError

import httpx_sse

from ..core.api_error import ApiError
from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.pydantic_utilities import pydantic_v1
from ..core.request_options import RequestOptions
from .errors.internal_server_error import InternalServerError
from .errors.unprocessable_entity_error import UnprocessableEntityError
from .types.chat_completion_chunk import ChatCompletionChunk
from .types.chat_completion_response import ChatCompletionResponse
from .types.chat_completion_response_format import ChatCompletionResponseFormat
from .types.chat_message import ChatMessage
from .types.completion_response import CompletionResponse
from .types.create_chat_completion_request_tool_choice import CreateChatCompletionRequestToolChoice
from .types.create_chat_completion_stream_request_tool_choice import CreateChatCompletionStreamRequestToolChoice
from .types.error_response import ErrorResponse
from .types.http_validation_error import HttpValidationError
from .types.prompt import Prompt
from .types.stop import Stop
from .types.stream_options import StreamOptions
from .types.tool_definition import ToolDefinition

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class TextGenClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def create_chat_completion_stream(
        self,
        *,
        messages: typing.Sequence[ChatMessage],
        model: str,
        frequency_penalty: typing.Optional[float] = OMIT,
        ignore_eos: typing.Optional[bool] = OMIT,
        logit_bias: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        loglikelihood: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[bool] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        n: typing.Optional[int] = OMIT,
        peft: typing.Optional[str] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        repetition_penalty: typing.Optional[float] = OMIT,
        response_format: typing.Optional[ChatCompletionResponseFormat] = OMIT,
        stop: typing.Optional[Stop] = OMIT,
        stream_options: typing.Optional[StreamOptions] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        tool_choice: typing.Optional[CreateChatCompletionStreamRequestToolChoice] = OMIT,
        tools: typing.Optional[typing.Sequence[ToolDefinition]] = OMIT,
        top_logprobs: typing.Optional[int] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        user: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> typing.Iterator[ChatCompletionChunk]:
        """
        Create a Chat Completion.

        Parameters
        ----------
        messages : typing.Sequence[ChatMessage]
            A list of messages comprising the conversation so far.

        model : str
            The identifier of the model to use.Can be a shared tenancy or custom model identifier.

        frequency_penalty : typing.Optional[float]
            Penalizes new tokens based on their frequency in the generated text so far.

        ignore_eos : typing.Optional[bool]
            Whether to ignore the EOS token and continue generating tokens after the EOS token is generated.

        logit_bias : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            Modify the likelihood of specified tokens appearing in the completion. Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. As an example, you can pass {'50256': -100} to prevent the <|endoftext|> token from being generated.

        loglikelihood : typing.Optional[bool]
            Return log probabilities for all prompt tokens excluding the first one from prefill step if True.

        logprobs : typing.Optional[bool]
            Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.

        max_tokens : typing.Optional[int]
            Maximum number of tokens to generate per output sequence.

        n : typing.Optional[int]
            Number of output sequences to return.

        peft : typing.Optional[str]
            Parameter-efficient fine-tuning ID.

        presence_penalty : typing.Optional[float]
            Penalizes new tokens based on whether they appear in the generated text so far.

        repetition_penalty : typing.Optional[float]
            Controls the likelihood of the model generating repeated texts.

        response_format : typing.Optional[ChatCompletionResponseFormat]
            Allows specification of a response format and associated schema that will constrain the LLM output to that structure. For example, using the `json_object` type allows you to provide a desired json schema for the output to follow.

        stop : typing.Optional[Stop]
            Generation stop condition.

        stream_options : typing.Optional[StreamOptions]
            If set, usageStats will be streamed on the last content-containing chunk.

        temperature : typing.Optional[float]
            Controls the randomness of the sampling.

        tool_choice : typing.Optional[CreateChatCompletionStreamRequestToolChoice]
            Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {"type": "function", "function": {"name": "my_function"}} forces the model to call that tool. none is the default when no tools are present. auto is the default if tools are present.

        tools : typing.Optional[typing.Sequence[ToolDefinition]]
            A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.

        top_logprobs : typing.Optional[int]
            An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.

        top_p : typing.Optional[float]
            Controls the cumulative probability of the top tokens to consider.

        user : typing.Optional[str]
            A unique identifier representing your end-user.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.Iterator[ChatCompletionChunk]


        Examples
        --------
        from octoai.client import OctoAI
        from octoai.text_gen import (
            ChatCompletionResponseFormat,
            ChatMessage,
            FunctionDefinition,
            StreamOptions,
            ToolCall,
            ToolDefinition,
        )

        client = OctoAI(
            api_key="YOUR_API_KEY",
        )
        response = client.text_gen.create_chat_completion_stream(
            frequency_penalty=1.1,
            ignore_eos=True,
            logit_bias={"string": {"key": "value"}},
            loglikelihood=True,
            logprobs=True,
            max_tokens=1,
            messages=[
                ChatMessage(
                    content="string",
                    role="string",
                    tool_calls=[ToolCall()],
                )
            ],
            model="string",
            n=1,
            peft="string",
            presence_penalty=1.1,
            repetition_penalty=1.1,
            response_format=ChatCompletionResponseFormat(
                schema={"string": {"key": "value"}},
                type="string",
            ),
            stop="string",
            stream_options=StreamOptions(
                include_usage=True,
            ),
            temperature=1.1,
            tool_choice={"key": "value"},
            tools=[
                ToolDefinition(
                    function=FunctionDefinition(
                        description="string",
                        name="string",
                        parameters={"string": {"key": "value"}},
                    ),
                    type={"key": "value"},
                )
            ],
            top_logprobs=1,
            top_p=1.1,
            user="string",
        )
        for chunk in response:
            yield chunk
        """
        with self._client_wrapper.httpx_client.stream(
            "v1/chat/completions",
            base_url=self._client_wrapper.get_environment().text_gen,
            method="POST",
            json={
                "frequency_penalty": frequency_penalty,
                "ignore_eos": ignore_eos,
                "logit_bias": logit_bias,
                "loglikelihood": loglikelihood,
                "logprobs": logprobs,
                "max_tokens": max_tokens,
                "messages": messages,
                "model": model,
                "n": n,
                "peft": peft,
                "presence_penalty": presence_penalty,
                "repetition_penalty": repetition_penalty,
                "response_format": response_format,
                "stop": stop,
                "stream_options": stream_options,
                "temperature": temperature,
                "tool_choice": tool_choice,
                "tools": tools,
                "top_logprobs": top_logprobs,
                "top_p": top_p,
                "user": user,
                "stream": True,
            },
            request_options=request_options,
            omit=OMIT,
        ) as _response:
            try:
                if 200 <= _response.status_code < 300:
                    _event_source = httpx_sse.EventSource(_response)
                    for _sse in _event_source.iter_sse():
                        try:
                            yield pydantic_v1.parse_obj_as(ChatCompletionChunk, json.loads(_sse.data))  # type: ignore
                        except:
                            pass
                    return
                _response.read()
                if _response.status_code == 422:
                    raise UnprocessableEntityError(
                        pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                    )
                if _response.status_code == 500:
                    raise InternalServerError(pydantic_v1.parse_obj_as(ErrorResponse, _response.json()))  # type: ignore
                _response_json = _response.json()
            except JSONDecodeError:
                raise ApiError(status_code=_response.status_code, body=_response.text)
            raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_chat_completion(
        self,
        *,
        messages: typing.Sequence[ChatMessage],
        model: str,
        frequency_penalty: typing.Optional[float] = OMIT,
        ignore_eos: typing.Optional[bool] = OMIT,
        logit_bias: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        loglikelihood: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[bool] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        n: typing.Optional[int] = OMIT,
        peft: typing.Optional[str] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        repetition_penalty: typing.Optional[float] = OMIT,
        response_format: typing.Optional[ChatCompletionResponseFormat] = OMIT,
        stop: typing.Optional[Stop] = OMIT,
        stream_options: typing.Optional[StreamOptions] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        tool_choice: typing.Optional[CreateChatCompletionRequestToolChoice] = OMIT,
        tools: typing.Optional[typing.Sequence[ToolDefinition]] = OMIT,
        top_logprobs: typing.Optional[int] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        user: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> ChatCompletionResponse:
        """
        Create a Chat Completion.

        Parameters
        ----------
        messages : typing.Sequence[ChatMessage]
            A list of messages comprising the conversation so far.

        model : str
            The identifier of the model to use.Can be a shared tenancy or custom model identifier.

        frequency_penalty : typing.Optional[float]
            Penalizes new tokens based on their frequency in the generated text so far.

        ignore_eos : typing.Optional[bool]
            Whether to ignore the EOS token and continue generating tokens after the EOS token is generated.

        logit_bias : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            Modify the likelihood of specified tokens appearing in the completion. Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. As an example, you can pass {'50256': -100} to prevent the <|endoftext|> token from being generated.

        loglikelihood : typing.Optional[bool]
            Return log probabilities for all prompt tokens excluding the first one from prefill step if True.

        logprobs : typing.Optional[bool]
            Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.

        max_tokens : typing.Optional[int]
            Maximum number of tokens to generate per output sequence.

        n : typing.Optional[int]
            Number of output sequences to return.

        peft : typing.Optional[str]
            Parameter-efficient fine-tuning ID.

        presence_penalty : typing.Optional[float]
            Penalizes new tokens based on whether they appear in the generated text so far.

        repetition_penalty : typing.Optional[float]
            Controls the likelihood of the model generating repeated texts.

        response_format : typing.Optional[ChatCompletionResponseFormat]
            Allows specification of a response format and associated schema that will constrain the LLM output to that structure. For example, using the `json_object` type allows you to provide a desired json schema for the output to follow.

        stop : typing.Optional[Stop]
            Generation stop condition.

        stream_options : typing.Optional[StreamOptions]
            If set, usageStats will be streamed on the last content-containing chunk.

        temperature : typing.Optional[float]
            Controls the randomness of the sampling.

        tool_choice : typing.Optional[CreateChatCompletionRequestToolChoice]
            Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {"type": "function", "function": {"name": "my_function"}} forces the model to call that tool. none is the default when no tools are present. auto is the default if tools are present.

        tools : typing.Optional[typing.Sequence[ToolDefinition]]
            A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.

        top_logprobs : typing.Optional[int]
            An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.

        top_p : typing.Optional[float]
            Controls the cumulative probability of the top tokens to consider.

        user : typing.Optional[str]
            A unique identifier representing your end-user.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ChatCompletionResponse


        Examples
        --------
        from octoai.client import OctoAI
        from octoai.text_gen import ChatMessage

        client = OctoAI(
            api_key="YOUR_API_KEY",
        )
        client.text_gen.create_chat_completion(
            messages=[
                ChatMessage(
                    role="role",
                )
            ],
            model="model",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "v1/chat/completions",
            base_url=self._client_wrapper.get_environment().text_gen,
            method="POST",
            json={
                "frequency_penalty": frequency_penalty,
                "ignore_eos": ignore_eos,
                "logit_bias": logit_bias,
                "loglikelihood": loglikelihood,
                "logprobs": logprobs,
                "max_tokens": max_tokens,
                "messages": messages,
                "model": model,
                "n": n,
                "peft": peft,
                "presence_penalty": presence_penalty,
                "repetition_penalty": repetition_penalty,
                "response_format": response_format,
                "stop": stop,
                "stream_options": stream_options,
                "temperature": temperature,
                "tool_choice": tool_choice,
                "tools": tools,
                "top_logprobs": top_logprobs,
                "top_p": top_p,
                "user": user,
                "stream": False,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(ChatCompletionResponse, _response.json())  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            if _response.status_code == 500:
                raise InternalServerError(pydantic_v1.parse_obj_as(ErrorResponse, _response.json()))  # type: ignore
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_completion_stream(
        self,
        *,
        model: str,
        best_of: typing.Optional[int] = OMIT,
        echo: typing.Optional[bool] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        ignore_eos: typing.Optional[bool] = OMIT,
        logit_bias: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        loglikelihood: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        n: typing.Optional[int] = OMIT,
        peft: typing.Optional[str] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        prompt: typing.Optional[Prompt] = OMIT,
        repetition_penalty: typing.Optional[float] = OMIT,
        seed: typing.Optional[int] = OMIT,
        stop: typing.Optional[Stop] = OMIT,
        stream_options: typing.Optional[StreamOptions] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        user: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> typing.Iterator[CompletionResponse]:
        """
        Parameters
        ----------
        model : str
            Model name to use for completion.

        best_of : typing.Optional[int]
            Number of sequences that are generated from the prompt.`best_of` must be greater than or equal to `n`.

        echo : typing.Optional[bool]
            Echo back the prompt in addition to the completion.

        frequency_penalty : typing.Optional[float]
            Penalizes new tokens based on their frequency in the generated text so far.

        ignore_eos : typing.Optional[bool]
            Whether to ignore the EOS token and continue generating tokens after the EOS token is generated.

        logit_bias : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            Modify the likelihood of specified tokens appearing in the completion.

        loglikelihood : typing.Optional[bool]
            Switch on loglikelihood regime and return log probabilities from all prompt tokens from prefill.

        logprobs : typing.Optional[int]
            Number of log probabilities to return per output token.

        max_tokens : typing.Optional[int]
            Maximum number of tokens to generate per output sequence.

        n : typing.Optional[int]
            Number of output sequences to return.

        peft : typing.Optional[str]
            Parameter-efficient fine-tuning ID.

        presence_penalty : typing.Optional[float]
            Penalizes new tokens based on whether they appear in the generated text so far.

        prompt : typing.Optional[Prompt]
            The prompt to generate completions from.

        repetition_penalty : typing.Optional[float]
            Controls the likelihood of the model generating repeated texts.

        seed : typing.Optional[int]
            If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.

        stop : typing.Optional[Stop]
            Strings that stop the generation when they are generated.

        stream_options : typing.Optional[StreamOptions]
            If set, usageStats will be streamed on the last content-containing chunk.

        suffix : typing.Optional[str]
            The suffix that comes after a completion of inserted text.

        temperature : typing.Optional[float]
            Controls the randomness of the sampling.

        top_p : typing.Optional[float]
            Controls the cumulative probability of the top tokens to consider.

        user : typing.Optional[str]
            A unique identifier representing your end-user.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.Iterator[CompletionResponse]


        Examples
        --------
        from octoai.client import OctoAI
        from octoai.text_gen import StreamOptions

        client = OctoAI(
            api_key="YOUR_API_KEY",
        )
        response = client.text_gen.create_completion_stream(
            best_of=1,
            echo=True,
            frequency_penalty=1.1,
            ignore_eos=True,
            logit_bias={"string": {"key": "value"}},
            loglikelihood=True,
            logprobs=1,
            max_tokens=1,
            model="string",
            n=1,
            peft="string",
            presence_penalty=1.1,
            prompt="string",
            repetition_penalty=1.1,
            seed=1,
            stop="string",
            stream_options=StreamOptions(
                include_usage=True,
            ),
            suffix="string",
            temperature=1.1,
            top_p=1.1,
            user="string",
        )
        for chunk in response:
            yield chunk
        """
        with self._client_wrapper.httpx_client.stream(
            "v1/completions",
            base_url=self._client_wrapper.get_environment().text_gen,
            method="POST",
            json={
                "best_of": best_of,
                "echo": echo,
                "frequency_penalty": frequency_penalty,
                "ignore_eos": ignore_eos,
                "logit_bias": logit_bias,
                "loglikelihood": loglikelihood,
                "logprobs": logprobs,
                "max_tokens": max_tokens,
                "model": model,
                "n": n,
                "peft": peft,
                "presence_penalty": presence_penalty,
                "prompt": prompt,
                "repetition_penalty": repetition_penalty,
                "seed": seed,
                "stop": stop,
                "stream_options": stream_options,
                "suffix": suffix,
                "temperature": temperature,
                "top_p": top_p,
                "user": user,
                "stream": True,
            },
            request_options=request_options,
            omit=OMIT,
        ) as _response:
            try:
                if 200 <= _response.status_code < 300:
                    _event_source = httpx_sse.EventSource(_response)
                    for _sse in _event_source.iter_sse():
                        try:
                            yield pydantic_v1.parse_obj_as(CompletionResponse, json.loads(_sse.data))  # type: ignore
                        except:
                            pass
                    return
                _response.read()
                if _response.status_code == 422:
                    raise UnprocessableEntityError(
                        pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                    )
                if _response.status_code == 500:
                    raise InternalServerError(pydantic_v1.parse_obj_as(ErrorResponse, _response.json()))  # type: ignore
                _response_json = _response.json()
            except JSONDecodeError:
                raise ApiError(status_code=_response.status_code, body=_response.text)
            raise ApiError(status_code=_response.status_code, body=_response_json)

    def create_completion(
        self,
        *,
        model: str,
        best_of: typing.Optional[int] = OMIT,
        echo: typing.Optional[bool] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        ignore_eos: typing.Optional[bool] = OMIT,
        logit_bias: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        loglikelihood: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        n: typing.Optional[int] = OMIT,
        peft: typing.Optional[str] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        prompt: typing.Optional[Prompt] = OMIT,
        repetition_penalty: typing.Optional[float] = OMIT,
        seed: typing.Optional[int] = OMIT,
        stop: typing.Optional[Stop] = OMIT,
        stream_options: typing.Optional[StreamOptions] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        user: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> CompletionResponse:
        """
        Parameters
        ----------
        model : str
            Model name to use for completion.

        best_of : typing.Optional[int]
            Number of sequences that are generated from the prompt.`best_of` must be greater than or equal to `n`.

        echo : typing.Optional[bool]
            Echo back the prompt in addition to the completion.

        frequency_penalty : typing.Optional[float]
            Penalizes new tokens based on their frequency in the generated text so far.

        ignore_eos : typing.Optional[bool]
            Whether to ignore the EOS token and continue generating tokens after the EOS token is generated.

        logit_bias : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            Modify the likelihood of specified tokens appearing in the completion.

        loglikelihood : typing.Optional[bool]
            Switch on loglikelihood regime and return log probabilities from all prompt tokens from prefill.

        logprobs : typing.Optional[int]
            Number of log probabilities to return per output token.

        max_tokens : typing.Optional[int]
            Maximum number of tokens to generate per output sequence.

        n : typing.Optional[int]
            Number of output sequences to return.

        peft : typing.Optional[str]
            Parameter-efficient fine-tuning ID.

        presence_penalty : typing.Optional[float]
            Penalizes new tokens based on whether they appear in the generated text so far.

        prompt : typing.Optional[Prompt]
            The prompt to generate completions from.

        repetition_penalty : typing.Optional[float]
            Controls the likelihood of the model generating repeated texts.

        seed : typing.Optional[int]
            If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.

        stop : typing.Optional[Stop]
            Strings that stop the generation when they are generated.

        stream_options : typing.Optional[StreamOptions]
            If set, usageStats will be streamed on the last content-containing chunk.

        suffix : typing.Optional[str]
            The suffix that comes after a completion of inserted text.

        temperature : typing.Optional[float]
            Controls the randomness of the sampling.

        top_p : typing.Optional[float]
            Controls the cumulative probability of the top tokens to consider.

        user : typing.Optional[str]
            A unique identifier representing your end-user.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CompletionResponse


        Examples
        --------
        from octoai.client import OctoAI

        client = OctoAI(
            api_key="YOUR_API_KEY",
        )
        client.text_gen.create_completion(
            model="model",
        )
        """
        _response = self._client_wrapper.httpx_client.request(
            "v1/completions",
            base_url=self._client_wrapper.get_environment().text_gen,
            method="POST",
            json={
                "best_of": best_of,
                "echo": echo,
                "frequency_penalty": frequency_penalty,
                "ignore_eos": ignore_eos,
                "logit_bias": logit_bias,
                "loglikelihood": loglikelihood,
                "logprobs": logprobs,
                "max_tokens": max_tokens,
                "model": model,
                "n": n,
                "peft": peft,
                "presence_penalty": presence_penalty,
                "prompt": prompt,
                "repetition_penalty": repetition_penalty,
                "seed": seed,
                "stop": stop,
                "stream_options": stream_options,
                "suffix": suffix,
                "temperature": temperature,
                "top_p": top_p,
                "user": user,
                "stream": False,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(CompletionResponse, _response.json())  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            if _response.status_code == 500:
                raise InternalServerError(pydantic_v1.parse_obj_as(ErrorResponse, _response.json()))  # type: ignore
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncTextGenClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def create_chat_completion_stream(
        self,
        *,
        messages: typing.Sequence[ChatMessage],
        model: str,
        frequency_penalty: typing.Optional[float] = OMIT,
        ignore_eos: typing.Optional[bool] = OMIT,
        logit_bias: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        loglikelihood: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[bool] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        n: typing.Optional[int] = OMIT,
        peft: typing.Optional[str] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        repetition_penalty: typing.Optional[float] = OMIT,
        response_format: typing.Optional[ChatCompletionResponseFormat] = OMIT,
        stop: typing.Optional[Stop] = OMIT,
        stream_options: typing.Optional[StreamOptions] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        tool_choice: typing.Optional[CreateChatCompletionStreamRequestToolChoice] = OMIT,
        tools: typing.Optional[typing.Sequence[ToolDefinition]] = OMIT,
        top_logprobs: typing.Optional[int] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        user: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> typing.AsyncIterator[ChatCompletionChunk]:
        """
        Create a Chat Completion.

        Parameters
        ----------
        messages : typing.Sequence[ChatMessage]
            A list of messages comprising the conversation so far.

        model : str
            The identifier of the model to use.Can be a shared tenancy or custom model identifier.

        frequency_penalty : typing.Optional[float]
            Penalizes new tokens based on their frequency in the generated text so far.

        ignore_eos : typing.Optional[bool]
            Whether to ignore the EOS token and continue generating tokens after the EOS token is generated.

        logit_bias : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            Modify the likelihood of specified tokens appearing in the completion. Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. As an example, you can pass {'50256': -100} to prevent the <|endoftext|> token from being generated.

        loglikelihood : typing.Optional[bool]
            Return log probabilities for all prompt tokens excluding the first one from prefill step if True.

        logprobs : typing.Optional[bool]
            Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.

        max_tokens : typing.Optional[int]
            Maximum number of tokens to generate per output sequence.

        n : typing.Optional[int]
            Number of output sequences to return.

        peft : typing.Optional[str]
            Parameter-efficient fine-tuning ID.

        presence_penalty : typing.Optional[float]
            Penalizes new tokens based on whether they appear in the generated text so far.

        repetition_penalty : typing.Optional[float]
            Controls the likelihood of the model generating repeated texts.

        response_format : typing.Optional[ChatCompletionResponseFormat]
            Allows specification of a response format and associated schema that will constrain the LLM output to that structure. For example, using the `json_object` type allows you to provide a desired json schema for the output to follow.

        stop : typing.Optional[Stop]
            Generation stop condition.

        stream_options : typing.Optional[StreamOptions]
            If set, usageStats will be streamed on the last content-containing chunk.

        temperature : typing.Optional[float]
            Controls the randomness of the sampling.

        tool_choice : typing.Optional[CreateChatCompletionStreamRequestToolChoice]
            Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {"type": "function", "function": {"name": "my_function"}} forces the model to call that tool. none is the default when no tools are present. auto is the default if tools are present.

        tools : typing.Optional[typing.Sequence[ToolDefinition]]
            A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.

        top_logprobs : typing.Optional[int]
            An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.

        top_p : typing.Optional[float]
            Controls the cumulative probability of the top tokens to consider.

        user : typing.Optional[str]
            A unique identifier representing your end-user.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.AsyncIterator[ChatCompletionChunk]


        Examples
        --------
        from octoai.client import AsyncOctoAI
        from octoai.text_gen import (
            ChatCompletionResponseFormat,
            ChatMessage,
            FunctionDefinition,
            StreamOptions,
            ToolCall,
            ToolDefinition,
        )

        client = AsyncOctoAI(
            api_key="YOUR_API_KEY",
        )
        response = await client.text_gen.create_chat_completion_stream(
            frequency_penalty=1.1,
            ignore_eos=True,
            logit_bias={"string": {"key": "value"}},
            loglikelihood=True,
            logprobs=True,
            max_tokens=1,
            messages=[
                ChatMessage(
                    content="string",
                    role="string",
                    tool_calls=[ToolCall()],
                )
            ],
            model="string",
            n=1,
            peft="string",
            presence_penalty=1.1,
            repetition_penalty=1.1,
            response_format=ChatCompletionResponseFormat(
                schema={"string": {"key": "value"}},
                type="string",
            ),
            stop="string",
            stream_options=StreamOptions(
                include_usage=True,
            ),
            temperature=1.1,
            tool_choice={"key": "value"},
            tools=[
                ToolDefinition(
                    function=FunctionDefinition(
                        description="string",
                        name="string",
                        parameters={"string": {"key": "value"}},
                    ),
                    type={"key": "value"},
                )
            ],
            top_logprobs=1,
            top_p=1.1,
            user="string",
        )
        async for chunk in response:
            yield chunk
        """
        async with self._client_wrapper.httpx_client.stream(
            "v1/chat/completions",
            base_url=self._client_wrapper.get_environment().text_gen,
            method="POST",
            json={
                "frequency_penalty": frequency_penalty,
                "ignore_eos": ignore_eos,
                "logit_bias": logit_bias,
                "loglikelihood": loglikelihood,
                "logprobs": logprobs,
                "max_tokens": max_tokens,
                "messages": messages,
                "model": model,
                "n": n,
                "peft": peft,
                "presence_penalty": presence_penalty,
                "repetition_penalty": repetition_penalty,
                "response_format": response_format,
                "stop": stop,
                "stream_options": stream_options,
                "temperature": temperature,
                "tool_choice": tool_choice,
                "tools": tools,
                "top_logprobs": top_logprobs,
                "top_p": top_p,
                "user": user,
                "stream": True,
            },
            request_options=request_options,
            omit=OMIT,
        ) as _response:
            try:
                if 200 <= _response.status_code < 300:
                    _event_source = httpx_sse.EventSource(_response)
                    async for _sse in _event_source.aiter_sse():
                        try:
                            yield pydantic_v1.parse_obj_as(ChatCompletionChunk, json.loads(_sse.data))  # type: ignore
                        except:
                            pass
                    return
                await _response.aread()
                if _response.status_code == 422:
                    raise UnprocessableEntityError(
                        pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                    )
                if _response.status_code == 500:
                    raise InternalServerError(pydantic_v1.parse_obj_as(ErrorResponse, _response.json()))  # type: ignore
                _response_json = _response.json()
            except JSONDecodeError:
                raise ApiError(status_code=_response.status_code, body=_response.text)
            raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_chat_completion(
        self,
        *,
        messages: typing.Sequence[ChatMessage],
        model: str,
        frequency_penalty: typing.Optional[float] = OMIT,
        ignore_eos: typing.Optional[bool] = OMIT,
        logit_bias: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        loglikelihood: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[bool] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        n: typing.Optional[int] = OMIT,
        peft: typing.Optional[str] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        repetition_penalty: typing.Optional[float] = OMIT,
        response_format: typing.Optional[ChatCompletionResponseFormat] = OMIT,
        stop: typing.Optional[Stop] = OMIT,
        stream_options: typing.Optional[StreamOptions] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        tool_choice: typing.Optional[CreateChatCompletionRequestToolChoice] = OMIT,
        tools: typing.Optional[typing.Sequence[ToolDefinition]] = OMIT,
        top_logprobs: typing.Optional[int] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        user: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> ChatCompletionResponse:
        """
        Create a Chat Completion.

        Parameters
        ----------
        messages : typing.Sequence[ChatMessage]
            A list of messages comprising the conversation so far.

        model : str
            The identifier of the model to use.Can be a shared tenancy or custom model identifier.

        frequency_penalty : typing.Optional[float]
            Penalizes new tokens based on their frequency in the generated text so far.

        ignore_eos : typing.Optional[bool]
            Whether to ignore the EOS token and continue generating tokens after the EOS token is generated.

        logit_bias : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            Modify the likelihood of specified tokens appearing in the completion. Accepts a JSON object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. As an example, you can pass {'50256': -100} to prevent the <|endoftext|> token from being generated.

        loglikelihood : typing.Optional[bool]
            Return log probabilities for all prompt tokens excluding the first one from prefill step if True.

        logprobs : typing.Optional[bool]
            Whether to return log probabilities of the output tokens or not. If true, returns the log probabilities of each output token returned in the content of message.

        max_tokens : typing.Optional[int]
            Maximum number of tokens to generate per output sequence.

        n : typing.Optional[int]
            Number of output sequences to return.

        peft : typing.Optional[str]
            Parameter-efficient fine-tuning ID.

        presence_penalty : typing.Optional[float]
            Penalizes new tokens based on whether they appear in the generated text so far.

        repetition_penalty : typing.Optional[float]
            Controls the likelihood of the model generating repeated texts.

        response_format : typing.Optional[ChatCompletionResponseFormat]
            Allows specification of a response format and associated schema that will constrain the LLM output to that structure. For example, using the `json_object` type allows you to provide a desired json schema for the output to follow.

        stop : typing.Optional[Stop]
            Generation stop condition.

        stream_options : typing.Optional[StreamOptions]
            If set, usageStats will be streamed on the last content-containing chunk.

        temperature : typing.Optional[float]
            Controls the randomness of the sampling.

        tool_choice : typing.Optional[CreateChatCompletionRequestToolChoice]
            Controls which (if any) tool is called by the model. none means the model will not call any tool and instead generates a message. auto means the model can pick between generating a message or calling one or more tools. required means the model must call one or more tools. Specifying a particular tool via {"type": "function", "function": {"name": "my_function"}} forces the model to call that tool. none is the default when no tools are present. auto is the default if tools are present.

        tools : typing.Optional[typing.Sequence[ToolDefinition]]
            A list of tools the model may call. Currently, only functions are supported as a tool. Use this to provide a list of functions the model may generate JSON inputs for. A max of 128 functions are supported.

        top_logprobs : typing.Optional[int]
            An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability. `logprobs` must be set to `true` if this parameter is used.

        top_p : typing.Optional[float]
            Controls the cumulative probability of the top tokens to consider.

        user : typing.Optional[str]
            A unique identifier representing your end-user.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ChatCompletionResponse


        Examples
        --------
        from octoai.client import AsyncOctoAI
        from octoai.text_gen import ChatMessage

        client = AsyncOctoAI(
            api_key="YOUR_API_KEY",
        )
        await client.text_gen.create_chat_completion(
            messages=[
                ChatMessage(
                    role="role",
                )
            ],
            model="model",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "v1/chat/completions",
            base_url=self._client_wrapper.get_environment().text_gen,
            method="POST",
            json={
                "frequency_penalty": frequency_penalty,
                "ignore_eos": ignore_eos,
                "logit_bias": logit_bias,
                "loglikelihood": loglikelihood,
                "logprobs": logprobs,
                "max_tokens": max_tokens,
                "messages": messages,
                "model": model,
                "n": n,
                "peft": peft,
                "presence_penalty": presence_penalty,
                "repetition_penalty": repetition_penalty,
                "response_format": response_format,
                "stop": stop,
                "stream_options": stream_options,
                "temperature": temperature,
                "tool_choice": tool_choice,
                "tools": tools,
                "top_logprobs": top_logprobs,
                "top_p": top_p,
                "user": user,
                "stream": False,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(ChatCompletionResponse, _response.json())  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            if _response.status_code == 500:
                raise InternalServerError(pydantic_v1.parse_obj_as(ErrorResponse, _response.json()))  # type: ignore
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_completion_stream(
        self,
        *,
        model: str,
        best_of: typing.Optional[int] = OMIT,
        echo: typing.Optional[bool] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        ignore_eos: typing.Optional[bool] = OMIT,
        logit_bias: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        loglikelihood: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        n: typing.Optional[int] = OMIT,
        peft: typing.Optional[str] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        prompt: typing.Optional[Prompt] = OMIT,
        repetition_penalty: typing.Optional[float] = OMIT,
        seed: typing.Optional[int] = OMIT,
        stop: typing.Optional[Stop] = OMIT,
        stream_options: typing.Optional[StreamOptions] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        user: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> typing.AsyncIterator[CompletionResponse]:
        """
        Parameters
        ----------
        model : str
            Model name to use for completion.

        best_of : typing.Optional[int]
            Number of sequences that are generated from the prompt.`best_of` must be greater than or equal to `n`.

        echo : typing.Optional[bool]
            Echo back the prompt in addition to the completion.

        frequency_penalty : typing.Optional[float]
            Penalizes new tokens based on their frequency in the generated text so far.

        ignore_eos : typing.Optional[bool]
            Whether to ignore the EOS token and continue generating tokens after the EOS token is generated.

        logit_bias : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            Modify the likelihood of specified tokens appearing in the completion.

        loglikelihood : typing.Optional[bool]
            Switch on loglikelihood regime and return log probabilities from all prompt tokens from prefill.

        logprobs : typing.Optional[int]
            Number of log probabilities to return per output token.

        max_tokens : typing.Optional[int]
            Maximum number of tokens to generate per output sequence.

        n : typing.Optional[int]
            Number of output sequences to return.

        peft : typing.Optional[str]
            Parameter-efficient fine-tuning ID.

        presence_penalty : typing.Optional[float]
            Penalizes new tokens based on whether they appear in the generated text so far.

        prompt : typing.Optional[Prompt]
            The prompt to generate completions from.

        repetition_penalty : typing.Optional[float]
            Controls the likelihood of the model generating repeated texts.

        seed : typing.Optional[int]
            If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.

        stop : typing.Optional[Stop]
            Strings that stop the generation when they are generated.

        stream_options : typing.Optional[StreamOptions]
            If set, usageStats will be streamed on the last content-containing chunk.

        suffix : typing.Optional[str]
            The suffix that comes after a completion of inserted text.

        temperature : typing.Optional[float]
            Controls the randomness of the sampling.

        top_p : typing.Optional[float]
            Controls the cumulative probability of the top tokens to consider.

        user : typing.Optional[str]
            A unique identifier representing your end-user.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.AsyncIterator[CompletionResponse]


        Examples
        --------
        from octoai.client import AsyncOctoAI
        from octoai.text_gen import StreamOptions

        client = AsyncOctoAI(
            api_key="YOUR_API_KEY",
        )
        response = await client.text_gen.create_completion_stream(
            best_of=1,
            echo=True,
            frequency_penalty=1.1,
            ignore_eos=True,
            logit_bias={"string": {"key": "value"}},
            loglikelihood=True,
            logprobs=1,
            max_tokens=1,
            model="string",
            n=1,
            peft="string",
            presence_penalty=1.1,
            prompt="string",
            repetition_penalty=1.1,
            seed=1,
            stop="string",
            stream_options=StreamOptions(
                include_usage=True,
            ),
            suffix="string",
            temperature=1.1,
            top_p=1.1,
            user="string",
        )
        async for chunk in response:
            yield chunk
        """
        async with self._client_wrapper.httpx_client.stream(
            "v1/completions",
            base_url=self._client_wrapper.get_environment().text_gen,
            method="POST",
            json={
                "best_of": best_of,
                "echo": echo,
                "frequency_penalty": frequency_penalty,
                "ignore_eos": ignore_eos,
                "logit_bias": logit_bias,
                "loglikelihood": loglikelihood,
                "logprobs": logprobs,
                "max_tokens": max_tokens,
                "model": model,
                "n": n,
                "peft": peft,
                "presence_penalty": presence_penalty,
                "prompt": prompt,
                "repetition_penalty": repetition_penalty,
                "seed": seed,
                "stop": stop,
                "stream_options": stream_options,
                "suffix": suffix,
                "temperature": temperature,
                "top_p": top_p,
                "user": user,
                "stream": True,
            },
            request_options=request_options,
            omit=OMIT,
        ) as _response:
            try:
                if 200 <= _response.status_code < 300:
                    _event_source = httpx_sse.EventSource(_response)
                    async for _sse in _event_source.aiter_sse():
                        try:
                            yield pydantic_v1.parse_obj_as(CompletionResponse, json.loads(_sse.data))  # type: ignore
                        except:
                            pass
                    return
                await _response.aread()
                if _response.status_code == 422:
                    raise UnprocessableEntityError(
                        pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                    )
                if _response.status_code == 500:
                    raise InternalServerError(pydantic_v1.parse_obj_as(ErrorResponse, _response.json()))  # type: ignore
                _response_json = _response.json()
            except JSONDecodeError:
                raise ApiError(status_code=_response.status_code, body=_response.text)
            raise ApiError(status_code=_response.status_code, body=_response_json)

    async def create_completion(
        self,
        *,
        model: str,
        best_of: typing.Optional[int] = OMIT,
        echo: typing.Optional[bool] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        ignore_eos: typing.Optional[bool] = OMIT,
        logit_bias: typing.Optional[typing.Dict[str, typing.Optional[float]]] = OMIT,
        loglikelihood: typing.Optional[bool] = OMIT,
        logprobs: typing.Optional[int] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        n: typing.Optional[int] = OMIT,
        peft: typing.Optional[str] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        prompt: typing.Optional[Prompt] = OMIT,
        repetition_penalty: typing.Optional[float] = OMIT,
        seed: typing.Optional[int] = OMIT,
        stop: typing.Optional[Stop] = OMIT,
        stream_options: typing.Optional[StreamOptions] = OMIT,
        suffix: typing.Optional[str] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        top_p: typing.Optional[float] = OMIT,
        user: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None
    ) -> CompletionResponse:
        """
        Parameters
        ----------
        model : str
            Model name to use for completion.

        best_of : typing.Optional[int]
            Number of sequences that are generated from the prompt.`best_of` must be greater than or equal to `n`.

        echo : typing.Optional[bool]
            Echo back the prompt in addition to the completion.

        frequency_penalty : typing.Optional[float]
            Penalizes new tokens based on their frequency in the generated text so far.

        ignore_eos : typing.Optional[bool]
            Whether to ignore the EOS token and continue generating tokens after the EOS token is generated.

        logit_bias : typing.Optional[typing.Dict[str, typing.Optional[float]]]
            Modify the likelihood of specified tokens appearing in the completion.

        loglikelihood : typing.Optional[bool]
            Switch on loglikelihood regime and return log probabilities from all prompt tokens from prefill.

        logprobs : typing.Optional[int]
            Number of log probabilities to return per output token.

        max_tokens : typing.Optional[int]
            Maximum number of tokens to generate per output sequence.

        n : typing.Optional[int]
            Number of output sequences to return.

        peft : typing.Optional[str]
            Parameter-efficient fine-tuning ID.

        presence_penalty : typing.Optional[float]
            Penalizes new tokens based on whether they appear in the generated text so far.

        prompt : typing.Optional[Prompt]
            The prompt to generate completions from.

        repetition_penalty : typing.Optional[float]
            Controls the likelihood of the model generating repeated texts.

        seed : typing.Optional[int]
            If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same `seed` and parameters should return the same result. Determinism is not guaranteed, and you should refer to the `system_fingerprint` response parameter to monitor changes in the backend.

        stop : typing.Optional[Stop]
            Strings that stop the generation when they are generated.

        stream_options : typing.Optional[StreamOptions]
            If set, usageStats will be streamed on the last content-containing chunk.

        suffix : typing.Optional[str]
            The suffix that comes after a completion of inserted text.

        temperature : typing.Optional[float]
            Controls the randomness of the sampling.

        top_p : typing.Optional[float]
            Controls the cumulative probability of the top tokens to consider.

        user : typing.Optional[str]
            A unique identifier representing your end-user.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CompletionResponse


        Examples
        --------
        from octoai.client import AsyncOctoAI

        client = AsyncOctoAI(
            api_key="YOUR_API_KEY",
        )
        await client.text_gen.create_completion(
            model="model",
        )
        """
        _response = await self._client_wrapper.httpx_client.request(
            "v1/completions",
            base_url=self._client_wrapper.get_environment().text_gen,
            method="POST",
            json={
                "best_of": best_of,
                "echo": echo,
                "frequency_penalty": frequency_penalty,
                "ignore_eos": ignore_eos,
                "logit_bias": logit_bias,
                "loglikelihood": loglikelihood,
                "logprobs": logprobs,
                "max_tokens": max_tokens,
                "model": model,
                "n": n,
                "peft": peft,
                "presence_penalty": presence_penalty,
                "prompt": prompt,
                "repetition_penalty": repetition_penalty,
                "seed": seed,
                "stop": stop,
                "stream_options": stream_options,
                "suffix": suffix,
                "temperature": temperature,
                "top_p": top_p,
                "user": user,
                "stream": False,
            },
            request_options=request_options,
            omit=OMIT,
        )
        try:
            if 200 <= _response.status_code < 300:
                return pydantic_v1.parse_obj_as(CompletionResponse, _response.json())  # type: ignore
            if _response.status_code == 422:
                raise UnprocessableEntityError(
                    pydantic_v1.parse_obj_as(HttpValidationError, _response.json())  # type: ignore
                )
            if _response.status_code == 500:
                raise InternalServerError(pydantic_v1.parse_obj_as(ErrorResponse, _response.json()))  # type: ignore
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)
